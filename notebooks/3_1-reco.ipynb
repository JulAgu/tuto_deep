{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation learning & recommender systems\n",
    "\n",
    "In this practical session, we investigate two classical matrix-factorization models and their neural network implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install torch torchvision pytorch-lightning --upgrade\n",
    "#! pip install matplotlib --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data used : [smallest movie-lens dataset](https://grouplens.org/datasets/movielens/)\n",
    "\n",
    "Let's start with a very common dataset describing users, movies & interactions (ratings):\n",
    "\n",
    "![image reco](media/Facto-mat.png)\n",
    "\n",
    "# 1)  Load & Prepare Data\n",
    "\n",
    "To be able to embed the data easily, we need to remap  the user/items between [0->N_User] and [0->N_Items]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "## Load\n",
    "ratings = pd.read_csv(\"data/ratings.csv\")\n",
    "ratings.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Prepare Data\n",
    "user_map = {user:num for num,user in enumerate(ratings[\"userId\"].unique())}\n",
    "item_map = {item:num for num,item in enumerate(ratings[\"movieId\"].unique())}\n",
    "\n",
    "## Number of users & items\n",
    "num_users = len(user_map)\n",
    "num_items = len(item_map)\n",
    "\n",
    "ratings[\"userId\"] = ratings[\"userId\"].map(user_map)\n",
    "ratings[\"movieId\"] = ratings[\"movieId\"].map(item_map)\n",
    "\n",
    "ratings.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating Test/Train as before\n",
    "\n",
    "train_indexes,val_indexes,test_indexes = [],[],[]\n",
    "\n",
    "for index in range(len(ratings)):\n",
    "    if index%5 == 0: # 20% of the data\n",
    "        test_indexes.append(index)\n",
    "    else:\n",
    "        train_indexes.append(index)\n",
    "\n",
    "        \n",
    "shuffle(train_indexes)\n",
    "num_val = int(len(train_indexes)/100*20)\n",
    "val_indexes = train_indexes[:num_val]\n",
    "train_indexes = train_indexes[num_val:]\n",
    "\n",
    "train_ratings = ratings.iloc[train_indexes].copy() # separate data\n",
    "val_ratings = ratings.iloc[val_indexes].copy()\n",
    "test_ratings = ratings.iloc[test_indexes].copy()\n",
    "\n",
    "\n",
    "print(f\" #train:{len(train_ratings)}, #val:{len(val_ratings)} ,#test:{len(test_ratings)}\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAGE\n",
    "# In what follows, we will browse the tuple this way:\n",
    "cpt = 0\n",
    "for index, uid, mid, r, ts in train_ratings.itertuples():\n",
    "    print(index,uid, mid,r) # remember that indexes were shuffled\n",
    "    cpt+=1\n",
    "    if cpt > 5:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce the baseline model with pytorch's vanilla autograd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal now is to reproduce the following (strong) baseline model from surprise\n",
    "\n",
    " $$\\hat{r}_{ui} = b_{ui} = \\mu + b_u + b_i$$\n",
    "\n",
    "[no matrix factorization here, only 3 scalars involved for a prediction $(u,i)$] <BR>\n",
    "[Even $\\mu$ could be computed from the train set, we are going to learn this parameter in the optimization process]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's define the parameters\n",
    "\n",
    "You have many parameters, they are all 1-dimensional:\n",
    "- **mu:** the global mean (1,)\n",
    "- **bu:** the user means (n_users,)\n",
    "- **bi:** the item means (n_items,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = torch.tensor([3.5],requires_grad=True) # activate gradient to be able to learn something\n",
    "bu = [torch.tensor([0.1],requires_grad=True) for _ in range(num_users)]\n",
    "bi = [torch.tensor([0.1],requires_grad=True) for _ in range(num_items)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then, we define two functions: \n",
    "\n",
    "- `predict(u,i)` : Will return the prediction given the (user,item) pair\n",
    "- `error(pred,real)` : Will return the MSE error of prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (TODO) Predict Function\n",
    "This function should implement this: $\\hat{r}_{ui} = b_{ui} = \\mu + b_u + b_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(u,i):\n",
    "    # build a (simlple) prediction from the above mentioned parameters\n",
    "    ##  TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) error function\n",
    "We want to use the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(pred,real):\n",
    "    # define simple MSE\n",
    "    ##  TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The evaluation loop, without any optimization for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_e = 0\n",
    "for index, uid, mid, r, ts in train_ratings.itertuples(): # elegant way to browse tuples (from pandas structure)\n",
    "    result = predict(uid,mid)\n",
    "    train_e += error(result,r).item()\n",
    "\n",
    "# define the same command for validation, test\n",
    "# display the errors    \n",
    "# The 3 errors are likely to be close\n",
    "##  TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's optimize the parameters (with SGD)  by slightly modifying the previous loop\n",
    "\n",
    "### (TODO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters' values\n",
    "lr = 0.01\n",
    "batch_size = 32\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # loop on the training samples\n",
    "    #   prediction\n",
    "    #   error\n",
    "    #   backward (accumulation)\n",
    "    #   if batch_size\n",
    "    #       update\n",
    "    #       zero_grad\n",
    "\n",
    "    #  TODO \n",
    "\n",
    "    # Evalaution on the validation set + test set\n",
    "    val_e = 0\n",
    "    for index, uid, mid, r, ts in val_ratings.itertuples():\n",
    "        result = predict(uid,mid)\n",
    "        val_e += error(result,r).item()\n",
    "\n",
    "    print(f\"epoch {epoch} val error : \", val_e/len(val_ratings))\n",
    "\n",
    "    test_e = 0\n",
    "    for index, uid, mid, r, ts in test_ratings.itertuples():\n",
    "        result = predict(uid,mid)\n",
    "        test_e += error(result,r).item()\n",
    "\n",
    "    print(f\"epoch {epoch} test error : \", test_e/len(test_ratings))\n",
    "    print(\"-----\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding module\n",
    "\n",
    "To build a matrix of vectorial representations of dimension $Z$, for instance describing the users, we are going to use a new module called `embedding`:\n",
    "$$ U = \\begin{pmatrix}\\mathbf u_1, \\ldots, \\mathbf u_n\\end{pmatrix}, \\mathbf u \\in \\mathbb R^Z $$ \n",
    "\n",
    "Call for a index, get a $Z$ dimensional representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 10\n",
    "nb_users = 100\n",
    "users = torch.nn.Embedding(nb_users, latent_size) # random init\n",
    "\n",
    "# get representation of user 5:\n",
    "print(\"User 5:\", users(torch.tensor(5))) # WARNING: call for a tensor (not an int)\n",
    "\n",
    "# get representation of user 5 & 7:\n",
    "print(\"User 5 & 7:\", users(torch.tensor([5,7])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding with smaller values:\n",
    "\n",
    "torch.nn.init.normal_(users.weight,0,0.01) # apply on the weights\n",
    "\n",
    "# get representation of user 5:\n",
    "print(\"User 5:\", users(torch.tensor(5))) # WARNING: call for a tensor (not an int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Classic matrix factorisation (called SVD in RecSys) (with mean)\n",
    "\n",
    "To see how it works, we propose to implement a simple SVD:\n",
    "### $$ \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu) }_\\text{regularization} $$\n",
    "\n",
    "where prediction is done in the following way:\n",
    "###Â $$r_{ui} = \\mu + U_u.I_i $$\n",
    "\n",
    "where $\\mu$ is the global mean,  $U_u$ a user embedding and $I_i$ an item embedding\n",
    "\n",
    "### STEPS:\n",
    " To implement such model in pytorch, we need to do multiple things:\n",
    " \n",
    " - (1) model definition\n",
    " - (2) loss function\n",
    " - (3) evaluation\n",
    " - (4) training/eval loop\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### (1) Model definition\n",
    "\n",
    "A model class typically extends `nn.Module`, the Neural network module. It is a convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.\n",
    "\n",
    "One should define two functions: `__init__` and `forward`.\n",
    "\n",
    "- `__init__` is used to initialize the model parameters\n",
    "- `forward` is the net transformation from input to output. In fact, when doing `moduleClass(input)` you call this method.\n",
    "\n",
    "##### (a) Initialization\n",
    "\n",
    "Our model has different weigths:\n",
    "\n",
    "- the user profiles (also called user embeddings) $U$\n",
    "- the item profiles (also called user embeddings) $I$\n",
    "- the mean bias $\\mu$\n",
    "\n",
    "\n",
    "##### (b) input to output operation\n",
    "Technically, the prediction as defined earlier can be seen as just a dot product between two embeddings $U_u$ and $I_i$ plus the mean rating:\n",
    "\n",
    "- `torch.sum(embed_u*embed_i,1) + self.mean` is equivalent to $r_{ui} = \\mu + U_u.I_i $ \n",
    "- the `.squeeze(1)` operation is a shape operation to remove the dimension 1 (indexing starts at 0) akin to reshaping the matrix from `(batch_size,1,latent_size)` to `(batch_size,latent_size)`\n",
    "- for reference, the inverse operation is `.unsqueeze()`\n",
    "- we return weights to regularize them\n",
    "\n",
    "\n",
    "### (TODO) Just to make sure you were following: complete the following `__init__`and  `forward` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# The model define as a class, inheriting from nn.Module\n",
    "class ClassicMF(torch.nn.Module):\n",
    "    \n",
    "    #(a) Init\n",
    "    def __init__(self,nb_users,nb_items,latent_size):\n",
    "        super(ClassicMF, self).__init__()\n",
    "        # define the embeddings\n",
    "        #   note: to define an attribute: self.users = ...\n",
    "        # initialize with std = 0.01\n",
    "        # define mu & initialize = 3\n",
    "\n",
    "        #  TODO \n",
    "    \n",
    "    # (b) How we compute the prediction (from input to output)\n",
    "    def forward(self, user, item): ## method called when doing ClassicMF(user,item)\n",
    "        # pay attention to the arguments: we have to give indexes\n",
    "        # from the indexes, compute the output\n",
    "        # WARNING: print self.users(user) once to understand which dimension to squeeze\n",
    "        # WARNING (2): return the embeddings on top of the output to compute the regularization term => 4 outputs expected\n",
    "\n",
    "        #  TODO \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2-4) full train loop\n",
    "\n",
    "The train loop is organized around the [Dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class which Combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset.\n",
    "\n",
    "We just redefine a collate function\n",
    "\n",
    "> collate_fn (callable, optional) â merges a list of samples to form a mini-batch.\n",
    "\n",
    "\n",
    "**NOTE:** The dataset argument can be a list instead of a \"Dataset\" instance (works by duck typing)\n",
    "    \n",
    "\n",
    "##### The train loop sequence is the following:\n",
    "    \n",
    "[Dataset ==Dataloader==> Batch (not prepared) ==collate_fn==> Batch (prepared) ==Model.forward==> Prediction =loss_fn=> loss <-> truth \n",
    "\n",
    "1] PREDICT\n",
    "- (a) The dataloader samples training exemples from the dataset (which is a list)\n",
    "- (b) The collate_fn prepares the minibatch of training exemples\n",
    "- (c) The prediction is made by feeding the minibatch in the model\n",
    "- (d) The loss is computed on the prediction via a loss function\n",
    "\n",
    "2] OPTIMIZE\n",
    "- (e) Gradients are computed by automatic backard propagation\n",
    "- (f) Parameters are updated using computed gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Let's create the datasets following  (Object w/ __getitem__(index) and __len()__, i.e lists ;)\n",
    "prep_train = [(tp.userId,tp.movieId,tp.rating) for tp in train_ratings.itertuples()]\n",
    "prep_val   = [(tp.userId,tp.movieId,tp.rating) for tp in val_ratings.itertuples()]\n",
    "prep_test  = [(tp.userId,tp.movieId,tp.rating) for tp in test_ratings.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = zip(*prep_train[:10])\n",
    "print(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# HyperParameters\n",
    "n_epochs = 3\n",
    "batch_size = 16\n",
    "num_feat = 25\n",
    "lr = 0.01\n",
    "reg = 0.001\n",
    "\n",
    "\n",
    "#(b) Collate function => Creates tensor batches to feed model during training\n",
    "# It can be removed if data is already tensors (torch or numpy ;)\n",
    "def tuple_batch(l):\n",
    "    '''\n",
    "    input l: list of (user,item,rating tuples)\n",
    "    output: formatted batches (in torch tensors)\n",
    "\n",
    "    takes n-tuples and create batch\n",
    "    text -> seq word #id\n",
    "    '''\n",
    "    users, items, ratings = zip(*l) \n",
    "    users_t = torch.LongTensor(users)\n",
    "    items_t = torch.LongTensor(items)\n",
    "    ratings_t = torch.FloatTensor(ratings)\n",
    "    \n",
    "    return users_t, items_t, ratings_t\n",
    "    \n",
    "\n",
    "\n",
    "#(d) Loss function => Combines MSE and L2\n",
    "def loss_func(pred,ratings_t,reg,*params): # specific syntax (cf details in the next box)\n",
    "    '''\n",
    "    mse loss combined with l2 regularization.\n",
    "    params assumed 2-dimension\n",
    "    '''\n",
    "    mse = F.mse_loss(pred,ratings_t,reduction='sum')\n",
    "    l2 = 0\n",
    "    for p in params: # ranging on all parameters\n",
    "        l2 += torch.mean(p.norm(2,-1))\n",
    "        \n",
    "    return (mse/pred.size(0)) + reg*l2 , mse\n",
    "    \n",
    "#\n",
    "# Training script starts here\n",
    "#    \n",
    "\n",
    "# (a) dataloader will sample data from datasets using collate_fn tuple_batch\n",
    "dataloader_train = DataLoader(prep_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_val = DataLoader(prep_val, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_test = DataLoader(prep_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=tuple_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model & optimizer\n",
    "\n",
    "model = ClassicMF(num_users,num_items,num_feat)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INTERMEDIATE BOX for in depth understanding\n",
    "\n",
    "# inference & parameter retrieving (if your forward is defined as expected)\n",
    "users_t,items_t,ratings_t = next(iter(dataloader_train)) # retrieve first batch\n",
    "# check dim\n",
    "print(users_t.size()) # batch\n",
    "print(users_t)\n",
    "\n",
    "# output of the forward step:\n",
    "pred, embed_u, embed_i, mu = model(users_t,items_t)\n",
    "print(pred.size(), embed_u.size()) # batch\n",
    "print(pred) # Current predictions for the batch\n",
    "\n",
    "# alternative advanced syntax\n",
    "pred, *params = model(users_t,items_t) # param is a list !!\n",
    "print(len(params)) \n",
    "print(params[0].size()) # params[0] corresponds to embed_u\n",
    "\n",
    "# idea: retrieving the list of parameter... And then transmit the list to loss_func without unpacking\n",
    "print(loss_func(pred,ratings_t,reg,*params))    # yhat, y, lambda_reg, all_params\n",
    "                                                # return mse + regul, mse (sum not the mean)\n",
    "\n",
    "# we can apply backward on what we want...\n",
    "                                                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Train loop (epoch)\n",
    "#   loop over the dataloader\n",
    "#       forward (+get the parameters)\n",
    "#       loss\n",
    "#       backward\n",
    "#       optim\n",
    "#   compute mse on validation & test\n",
    "#   display losses for epoch e\n",
    "#\n",
    "\n",
    "## TODO \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Your turn from scratch) Koren 2009 model:\n",
    "\n",
    "Here, this model simply adds a bias for each user and for each item\n",
    "\n",
    "### $$ \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu+ \\mu_i+\\mu_u))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu  + \\mu+ \\mu_i+\\mu_u) }_\\text{regularization} $$\n",
    "\n",
    "\n",
    "### $$r_{ui} = \\mu + \\mu_i + \\mu_u + U_u.I_i $$\n",
    "\n",
    "### TODO:\n",
    "\n",
    "- (a) complete the model initialization\n",
    "- (b) complete the forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) Here, train loop stays the same, you only have to change the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 16\n",
    "num_feat = 25\n",
    "lr = 0.01\n",
    "reg = 0.001\n",
    "\n",
    "# note: previous loss function should be robust to the new model thanks to advanced syntax :)\n",
    "\n",
    "model =  KorenMF(num_users,num_items,num_feat)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# same loop as before\n",
    "#  TODO \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du sujet Ã  partir de la correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  TODO )\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
