{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle d'attention\n",
    "\n",
    "Le mécanisme d'attention est une sorte d'apprentissage du pooling ou plutot de la moyenne: l'idée est de pondérer chaque élément de la somme avec un poids issu de paramètres appris.\n",
    "\n",
    "Ce TP envisage 4 formes d'attention, après avoir testé un modèle de base uniforme:\n",
    "1. Modèle uniforme: il s'agit d'un pooling classique (qui sera codé comme une attention uniforme)\n",
    "2. Attention simple: apprentissage d'un vecteur de pondération des éléments\n",
    "3. Attention personnalisée : les mots définissent leurs fonctions d'attention\n",
    "4. RNN et attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import os\n",
    "import click\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import logging\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classe de gestion des données textuelles\n",
    "\n",
    "1. Récupération d'embedding glove\n",
    "    1. Téléchargement:\n",
    "    ```wget http://nlp.stanford.edu/data/glove.6B.zip```\n",
    "    2. Lecture des fichiers\n",
    "2. Récupération des données imdb (classification d'opinion)\n",
    "3. Traitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recuperation des embbeding \n",
    "EMB_SIZE = 50 # 100, 200 or 300\n",
    "PATH = \"./data/glove/glove.6B/\" # répertoire où vous avez récupéré les embeddings\n",
    "\n",
    "vocab,embeddings = [],[]\n",
    "with open(PATH+'glove.6B.{:d}d.txt'.format(EMB_SIZE),'rt') as fi:\n",
    "    full_content = fi.read().strip().split('\\n')\n",
    "for i in range(len(full_content)):\n",
    "    i_word = full_content[i].split(' ')[0]\n",
    "    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n",
    "    vocab.append(i_word)\n",
    "    embeddings.append(i_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab[0]) # premier mot\n",
    "print(len(embeddings[0]), embeddings[0]) # premier embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération via huggingface des données imdb\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# dataset[\"train\"][0]\n",
    "print(dataset[\"train\"][0]['text'])\n",
    "print(dataset[\"train\"][0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FolderText(Dataset):\n",
    "    \"\"\"Dataset gérant la tokenization des documents à la volée\"\"\"\n",
    "\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.txts   = [data[i][\"text\"] for i in range(len(data))]\n",
    "        self.labels = [data[i][\"label\"] for i in range(len(data))]\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        return self.tokenizer(self.txts[ix]), self.labels[ix]\n",
    "    def get_txt(self,ix):\n",
    "        # s = self.txts[ix]\n",
    "        return self.txts[ix], self.labels[ix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mise en forme du dataset\n",
    "WORDS = re.compile(r\"\\S+\")\n",
    "\n",
    "embedding_size = len(embeddings[0])\n",
    "OOVID = len(vocab)\n",
    "vocab.append(\"__OOV__\")\n",
    "word2id = {word: ix for ix, word in enumerate(vocab)}\n",
    "embeddings = np.vstack((embeddings, np.zeros(embedding_size)))\n",
    "\n",
    "def tokenizer(t):\n",
    "    return [word2id.get(x, OOVID) for x in re.findall(WORDS, t.lower())]\n",
    "\n",
    "logging.info(\"Loading embeddings\")\n",
    "logging.info(\"Get the IMDB dataset\")\n",
    "\n",
    "train_data, test_data=FolderText(dataset[\"train\"], tokenizer), FolderText(dataset[\"test\"], tokenizer)\n",
    "id2word = dict((v, k) for k, v in word2id.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vérification rapide du bon fonctionnemnet des éléments ci-dessus\n",
    "\n",
    "sent = \"this movie was great\"\n",
    "ind = tokenizer(sent)\n",
    "print(ind)\n",
    "print(\"check reconstruction :\", \" \".join([id2word[i] for i in ind]))\n",
    "\n",
    "# avec un mot inconnu\n",
    "\n",
    "sent = \"this movie was qslkjgf\"\n",
    "ind = tokenizer(sent)\n",
    "print(ind)\n",
    "print(\"check reconstruction :\", \" \".join([id2word[i] for i in ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modélisation\n",
    "\n",
    "1. Jouer avec la fonction ```softmax```\n",
    "2. Construction d'un modèle de base\n",
    "\n",
    "\n",
    "Note: à chaque étape, on aurait pu/du implémenter un masque sur les séquences pour tenir compte du padding (d'où la fonction ci-dessous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def masked_softmax(x,lens=None):\n",
    "#     #X : B x N\n",
    "#     x = x.view(x.size(0),x.size(1))\n",
    "#     if lens is None:\n",
    "#         lens = torch.zeros(x.size(0),1).fill_(x.size(1))\n",
    "#     mask  = torch.arange(x.size(1),device=x.device).view(1,-1) < lens.view(-1,1)\n",
    "#     x[~mask] = float('-inf') # indices pas dans le mask\n",
    "#     # print(\"MASK\", lens,\"\\n\", mask,\"\\n\", x)\n",
    "#     return x.softmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explication des outils\n",
    "# la fonction softmax\n",
    "x = torch.tensor([[10, 1, 0.5, 0.2, 0, 0]])\n",
    "print(x)\n",
    "print(x.softmax(1)) # exp(x_i) / sum(exp(x_i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBase(nn.Module):\n",
    "    def __init__(self,embeddings,label_count):\n",
    "        super().__init__()\n",
    "        self.emb_layer = nn.Embedding.from_pretrained(embeddings)\n",
    "        self.linear = nn.Linear(embeddings.size(1),label_count)\n",
    "\n",
    "    def emb(self,x):\n",
    "        # question triviale: juste pour s'assurer que vous avez bien défini la fonction\n",
    "        #  TODO \n",
    "        \n",
    "\n",
    "    def forward(self,x): #B x N x E\n",
    "        #  TODO \n",
    "        return yhat\n",
    "    \n",
    "\n",
    "    def attention(self,x):\n",
    "        # retourner une attention uniforme (ATTENTION à la longueur variable des phrases)\n",
    "        #  TODO \n",
    "        return a.softmax(1) # B x N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Learner:\n",
    "    \"\"\"Base class for supervised learning\"\"\"\n",
    "\n",
    "    def __init__(self, model, model_id: str):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.optim = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "        self.model_id = model_id\n",
    "        self.iteration = 0\n",
    "\n",
    "    def run(self,train_loader, test_loader, epochs, test_iterations, device,entropy_pen=0.):\n",
    "        \"\"\"Run a model during `epochs` epochs\"\"\"\n",
    "        writer = SummaryWriter(f\"/tmp/runs/{self.model_id}\")\n",
    "        model = self.model.to(device)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        loss_nagg = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "        model.train()\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            # Iterate over batches\n",
    "            for x, y, lens in train_loader:\n",
    "                self.optim.zero_grad()\n",
    "                yhat = model(x.to(device))\n",
    "                l = loss(yhat, y.to(device))\n",
    "                probs = model.attention(model.emb(x.to(device))) # calcul de l'entropie\n",
    "                entrop = -(probs*(probs+1e-10).log()).sum(1).mean()\n",
    "                total_l = l+entropy_pen*entrop\n",
    "                total_l.backward()\n",
    "                self.optim.step()\n",
    "                writer.add_scalar('loss/train', l, self.iteration)\n",
    "                writer.add_scalar('loss/entrop',entrop,self.iteration)\n",
    "                writer.add_scalar('loss/total_train',total_l,self.iteration)\n",
    "                self.iteration += 1\n",
    "                \n",
    "                if self.iteration % test_iterations == 0:\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        lst_probs = []\n",
    "                        cumloss = 0\n",
    "                        cumcorrect = 0\n",
    "                        count = 0\n",
    "                        for x, y, lens in test_loader:\n",
    "                            yhat = model(x.to(device))\n",
    "                            cumloss += loss_nagg(yhat, y.to(device))\n",
    "                            cumcorrect += (yhat.argmax(1) == y.to(device)).sum()\n",
    "                            count += x.shape[0]\n",
    "                            probs =  model.attention(model.emb(x.to(device)))\n",
    "                            lst_probs.append(-(probs*(probs+1e-10).log()).sum(1))\n",
    "\n",
    "                        writer.add_scalar(\n",
    "                            'loss/test', cumloss.item() / count, self.iteration)\n",
    "                        writer.add_scalar(\n",
    "                            'correct/test', cumcorrect.item() / count, self.iteration)\n",
    "                        \n",
    "                        writer.add_histogram(f'entropy',torch.cat(lst_probs),self.iteration)\n",
    "                        \n",
    "                    model.train()\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "        \"\"\" Collate function for DataLoader \"\"\"\n",
    "        data = [torch.LongTensor(item[0]) for item in batch]\n",
    "        lens = [len(d) for d in data]\n",
    "        labels = [item[1] for item in batch]\n",
    "        return torch.nn.utils.rnn.pad_sequence(data, batch_first=True,padding_value = PAD), \\\n",
    "                torch.LongTensor(labels), torch.Tensor(lens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 30 # nepoch\n",
    "NITER = 50 # calcul perf test\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "PAD = word2id[\"__OOV__\"] # variable globale pour collate\n",
    "embeddings = torch.Tensor(embeddings)\n",
    "model = ModelBase(embeddings, 2) # deux étiquettes\n",
    "# model1 = ModelAttention1(embeddings, 2)\n",
    "# model2 = ModelAttention2(embeddings, 2)\n",
    "# model4 = ModelAttention4(embeddings, 2)\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True,batch_size=BATCH_SIZE, collate_fn=collate)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=collate,shuffle=False)\n",
    "\n",
    "learner = Learner(model, time.asctime())\n",
    "learner.run(train_loader,test_loader,EPOCH,NITER,device,0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploitation des résultats\n",
    "\n",
    "1. Visualiser l'attention sur les textes\n",
    "2. Calculer les performances et proposer des nouvelles phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize(words, color_array):\n",
    "    # words is a list of words\n",
    "    # color_array is an array of numbers between 0 and 1 of length equal to words\n",
    "    cmap = matplotlib.colormaps['Reds']\n",
    "    template = '<span class=\"barcode\"; style=\"color: black; background-color: {}\">{}</span>'\n",
    "    colored_string = ''\n",
    "    for word, color in zip(words, color_array):\n",
    "        color = matplotlib.colors.rgb2hex(cmap(color)[:3])\n",
    "        colored_string += template.format(color, '&nbsp' + word + '&nbsp')\n",
    "    return colored_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# mise en forme du texte de sortie:\n",
    "sent = \"this movie was great\"\n",
    "# att  = np.ones(len(sent.split(\" \"))) # np.random.rand(len(sent.split(\" \")))\n",
    "att  =  np.random.rand(len(sent.split(\" \")))\n",
    "print(att)\n",
    "display(HTML(colorize(sent.split(\" \"),att)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essai sur une phrase \n",
    "from IPython.display import display, HTML\n",
    "\n",
    "mod = model # selection du modèle\n",
    "\n",
    "sent = \"this movie was not so bad\"\n",
    "ind = tokenizer(sent)\n",
    "print(\"check :\", \" \".join([id2word[i] for i in ind]))\n",
    "ind = torch.tensor(ind).unsqueeze(0)\n",
    "print(ind)\n",
    "res = mod(ind.to(device))\n",
    "att = mod.attention(mod.emb(ind.to(device)))\n",
    "print(res)\n",
    "print(att)\n",
    "\n",
    "display(HTML(colorize(sent.split(\" \"),att.to(\"cpu\").detach().squeeze().numpy())))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  TODO )\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
