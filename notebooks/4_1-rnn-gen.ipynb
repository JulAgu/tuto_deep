{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 : RNN et attention pour le texte \n",
    "\n",
    "\n",
    "Vincent Guigue, inspiré de\n",
    "Nicolas Baskiotis (nicolas.baskiotis@isir.upmc.fr) Benjamin Piwowarski (benjamin@piwowarski.fr)  -- MLIA/LIP6, ISIR, Sorbonne Université"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La représentation moyenne étudiée au module précédent trouve vite ses limites lorsque l'on s'attaque à des tâches plus complexes (traduction, résumé automatique, ...) où ce qui nous intéresse est à une fine granularité dans le texte. Ce module explore les architectures récurrentes qui permettent de bien mieux traiter le texte. \n",
    "\n",
    "Nous allons étudier la *génération libre de texte* en utilisant `IMDB`.\n",
    "\n",
    "La génération de séquence consiste  à produire  une séquence de symboles discrets à partir d'une séquence en entrée. L'objectif est donc de décoder à partir de l'état caché une distribution  de probabilités multinomiale sur les symboles à engendrer. Il faut donc que la  dimension de sortie du RNN (du décodeur) soit égale au nombre de symboles considérés. Il faut par ailleurs utiliser un *softmax* pour obtenir une distribution à partir du décodage de l'état caché. Le coût cross-entropie est adapté pour apprendre cette distribution.\n",
    "\n",
    "Une fois le réseau appris, la génération se fait (soit à partir d'un début de séquence, soit à partir d'un état initial vide) en choisissant le symbole le plus probable dans la distribution multinomiale décodée à chaque pas de temps. Ce symbole est ensuite considéré comme entrée au pas de temps suivant et la génération se poursuit itérativement. Une autre possibilité est d'échantillonner suivant la multinomiale pour obtenir plusieurs échantillons. La génération se poursuit jusqu'à produire le token *\\<eos\\>*.\n",
    "\n",
    "La supervision se fait à chaque pas de temps (contrairement à la classification où la supervision se fait au dernier pas de temps) : c'est un exemple de réseau many-to-many. On veut inciter le réseau à produire à l'instant $t+1$ le token suivant de notre séquence. Ainsi à tout pas de temps $t$ on connaît la supervision. Le gradient du coût à propager est la somme de tous les coûts à chaque instant. Il ne faut pas oublier cependant à ne pas prendre en compte les tokens de padding dans le calcul du coût."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from typing import Iterable, List, Tuple\n",
    "\n",
    "import shutil\n",
    "import torchtext\n",
    "assert version.Version(torchtext.__version__) >= version.Version(\"0.9.0\")\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab, FastText\n",
    "from collections import Counter\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import sentencepiece as spm\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "\n",
    "cachepath = os.path.expanduser('~/.local/data')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BASEPATH = Path(\"xp/generation\")\n",
    "TB_PATH =  BASEPATH / \"logs\"\n",
    "TB_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "display(HTML(\"<h2>Informations</h2><div>Pour visualiser les logs, tapez la commande : </div>\"))\n",
    "print(f\"tensorboard --logdir {Path(TB_PATH).absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement et pré-traitement des données (XSum)\n",
    "\n",
    "Ci-dessous les fonctions qui permettent de charger les données et de les préparer (en lots/batchs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"xsum\", split={\"train\": \"train[:10%]\", \"validation\": \"validation[:5%]\", \"test\": \"validation[5%:]\"})\n",
    "metric = load_metric(\"rouge\")\n",
    "\n",
    "# Affiche les champs \n",
    "raw_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "# Extrait du jeu de données\n",
    "\n",
    "Voici un extrait aléatoire du jeu de données, avec un document (à résumé) et le résumé attendu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13"
   },
   "outputs": [],
   "source": [
    "print(raw_datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réduction du vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les réseaux récurrents sont lourds à mettre en oeuvre : pour chaque mini-batch il faut itérer sur la longueur des séquences. Il est donc judicieux de réduire autant que possible les dimensionalités à traiter.\n",
    " \n",
    "Le pré-traitement des textes repose sur une étape de segmentation où le texte est découpé en unités linguistiques. Pendant longtemps le niveau choisi était le mot (= chaîne alphanumérique entourée d'espace); depuis quelques années, des alternatives ont été (ré)explorées avec les nouveaux modèles neuronaux. \n",
    "\n",
    "Une des segmentations les plus efficaces à l'heure actuelle est le découpage en n-grammes variables (**subword units**) popularisé par le Byte-Pair Encoding (BPE) en 2016. Ces segmentations ont l'avantage d'avoir un vocabulaire de taille fixe qui couvre au mieux le jeu de données, et permet d'éviter le problème des mots inconnus.\n",
    "\n",
    "Par exemple, *You shoulda got David Carr of Third Day to do it* sera segmenté en ```\"\\_You\", \"\\_should\", \"a\", \"\\_got\", \"\\_D\", \"av\", \"id\", \"\\_C\", \"ar\", \"r\", \"\\_of\", \"\\_Th\", \"ir\", \"d\", \"\\_Day\", \"\\_to\", \"\\_do\", \"\\_it\"```\n",
    "\n",
    "où les séquences fréquentes (ex. You, should) sont extraites directement alors que des séquences moins fréquentes (ex. David, Carr) sont segmentées en plusieurs parties.\n",
    "\n",
    "La librairie [sentencepiece](https://github.com/google/sentencepiece/blob/master/python/README.md)</a> permet une telle segmentation. Les tokens Unknown *\\<unk\\>*, *BOS* (begin of sequence, *\\<s\\>*) and *EOS* (end of sequence, *\\</s\\>*) sont prédéfinis, mais vous pouvez en ajouter d'autres avec **user_defined_symbols**.\n",
    "    \n",
    "Nous allons également considéré qu'un sous-échantillon des exemples pour rendre plus rapide l'apprentissage.\n",
    "\n",
    "Ne pas oublier de préfixer chaque phrase par le token *\\<bos\\>* et le suffixé par *\\<eos\\>*, cela nous servira dans des tâches futures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La suite est classique : remarquez dans **pad_sequence** que cette fois on ne passe pas l'argument **batch_first=True** pour garder les conventions usuelles RNNs : la première dimension du tenseur est la longueur, la deuxième la taille du batch, la troisième la dimension d'embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def process_batch(self, batch: List[str]):            \n",
    "        return pad_sequence([self.string2idx(text) for text in batch])\n",
    "\n",
    "class SentencePiecePreprocessor(Preprocessor):\n",
    "    \"\"\"Tokenizer Sentence Piece\"\"\"\n",
    "    def embeddings(self, dimension: int):\n",
    "        return nn.Embedding(self.vocab_size, dimension, padding_idx=self.pad_index)\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return self.spm_tokenizer.vocab_size()\n",
    "\n",
    "    def __init__(self, name: str, sentences: Iterable[str], vocab_size=1000, force=False):\n",
    "        modelpath = BASEPATH / f\"{name}-{vocab_size}.model\"\n",
    "        if not modelpath.exists() or force:\n",
    "            print(f\"Entraînement de SPM, sortie {BASEPATH}/{name}-{vocab_size}\", flush=True)\n",
    "            spm.SentencePieceTrainer.train(\n",
    "                sentence_iterator=iter(sentences), \n",
    "                model_prefix=f\"{BASEPATH}/{name}-{vocab_size}\", \n",
    "                vocab_size=vocab_size,\n",
    "                pad_id=0,                \n",
    "                unk_id=1,\n",
    "                bos_id=2,\n",
    "                eos_id=3\n",
    "            )\n",
    "            \n",
    "        self.spm_tokenizer = spm.SentencePieceProcessor()\n",
    "        self.spm_tokenizer.load(str(modelpath))\n",
    "\n",
    "        self.pad_index = self.spm_tokenizer.pad_id()\n",
    "        self.eos_index = self.spm_tokenizer.eos_id()\n",
    "        self.bos_index = self.spm_tokenizer.bos_id()\n",
    "        self.oov_index = None\n",
    "\n",
    "        self.spm_tokenizer.SetEncodeExtraOptions(\"bos:eos\")\n",
    "\n",
    "    def string2idx(self, s: str) -> torch.Tensor:\n",
    "        return torch.tensor(self.spm_tokenizer.EncodeAsIds(s))\n",
    "\n",
    "    def tokenizer(self, x):\n",
    "        \"\"\"Segmentation du texte en sous-mots\"\"\"\n",
    "        return self.spm_tokenizer.encode_as_pieces(x)\n",
    "\n",
    "    def id2token(self, ix):\n",
    "        return self.spm_tokenizer.IdToPiece(ix)\n",
    "\n",
    "    def decode(self, ids: List[int]):\n",
    "        return self.spm_tokenizer.Decode(ids)\n",
    "\n",
    "\n",
    "# Si vous voulez essayer avec des mots\n",
    "# preprocessor = FastTextProcessor()\n",
    "\n",
    "preprocessor = SentencePiecePreprocessor(\"spm\", (row[\"summary\"] for row in raw_datasets[\"train\"]), force=False)\n",
    "print(f\"Vocabulary size: {preprocessor.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On essaie notre nouveau tokenizer sur \"hello world\" pour voir ce qui se passe. Notez :\n",
    "\n",
    "1. Le découpage en sous-mots\n",
    "1. La présence de tokens spéciaux (début `<s>` et fin de séquence `</s>`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.tokenizer(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "et maintenant, on traite notre jeu de données en entier avec la fonction `map` de datasets et une fonction de pré-traitement `preprocess_fn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"alert-success\"> Exercice : RNN simple </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le RNN utilisé pour la génération est composé :\n",
    "* d'une [couche d'embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) pour la représentation des tokens\n",
    "* d'une <a href=https://pytorch.org/docs/stable/generated/torch.nn.RNN.html> cellule RNN</a>\n",
    "* et d'un *décodeur*, chargé de décoder l'état latent du RNN vers la classe *positive* ou *négative*, sous la forme d'un réseau linéaire.\n",
    "\n",
    "Le **forward** d'un module **RNN** retourne deux tenseurs, le premier correspond au tenseur **output** de  taille `Longueur x Batch x Taille du vocabulaire`, le deuxième au tenseur **hidden** qui correspond juste au dernier état caché. Ce tenseur du dernier état caché est très utile lorsque l'on veut poursuivre l'inférence à partir de l'état de sortie pour initialiser l'état caché du réseau (cf fonction `generate`).\n",
    "\n",
    "Dans la cellule suivante, la boucle d'apprentissage (teacher forcing) est donnée. Vous pouvez modifier le code, si vous le souhaitez, pour utiliser des techniques d'apprentissage plus évoluées (utilisation de la sortie du RNN plutôt que de la vérité de terrain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCHSIZE = 128\n",
    "TEST_BATCHSIZE = 512\n",
    "\n",
    "def train_1to1(preprocessor, model, epochs, raw_datasets):\n",
    "    \"\"\"Fonction d'entraînement (teacher forcing)\"\"\"\n",
    "    print(f\"Training {model.name}\")\n",
    "    \n",
    "    # On nettoie le rep. de log\n",
    "    shutil.rmtree(f\"{TB_PATH}/{model.name}\", ignore_errors = True)\n",
    "    writer = SummaryWriter(f\"{TB_PATH}/{model.name}\")\n",
    "    \n",
    "    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_loader = DataLoader(raw_datasets[\"train\"], TRAIN_BATCHSIZE, shuffle=True)\n",
    "    test_loader = DataLoader(raw_datasets[\"test\"], TEST_BATCHSIZE, shuffle=False)\n",
    "    loss = nn.CrossEntropyLoss(ignore_index=preprocessor.pad_index)\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        cumloss, count =  0, 0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optim.zero_grad()\n",
    "            x = preprocessor.process_batch(batch[\"summary\"]).to(device)\n",
    "        \n",
    "            # Mode \"Teacher forcing\"\n",
    "            x, y = x[:-1], x[1:]\n",
    "            yhat, hidden = model.forward(x)\n",
    "            l = loss(yhat.view(-1,yhat.size(2)), y.view(-1))\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l*len(x)\n",
    "            count += len(x)\n",
    "        writer.add_scalar('loss/train', cumloss/count, epoch)\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                cumloss, count = 0, 0\n",
    "                for x in test_loader:\n",
    "                    x = preprocessor.process_batch(batch[\"summary\"])\n",
    "                    x, y = x[:-1].to(device), x[1:].to(device)\n",
    "                    yhat, hidden = model(x)\n",
    "                    cumloss += loss(yhat.view(-1,yhat.size(2)),y.view(-1))*len(x)\n",
    "                    count += len(x)\n",
    "                writer.add_scalar(f'loss/test',cumloss/count,epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Donnez l'implémentation du module RNN\n",
    "\n",
    "Il ne vous reste plus qu'à définir l'initialisation des modules (`__init__`) ainsi que leur utilisation (`forward`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.name = f\"rnn-{embeddings.embedding_dim}-{hidden_dim}\"\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "        # definir la partie recurrente dim_embedding => dim_h\n",
    "        # prediction de char: le predicteur doit revenir dans l'espace des embeddings\n",
    "        #  TODO \n",
    "        \n",
    "    \n",
    "    def forward(self, x, h_0=None):\n",
    "        #  TODO \n",
    "        # Attention de bien prendre en compte h_0 dans l'appel du RRN\n",
    "        \n",
    "\n",
    "# On récupère des embeddings pour le modèle (dimension 50)\n",
    "embeddings = preprocessor.embeddings(50)\n",
    "\n",
    "# Création du RNN avec 100 états cachés\n",
    "rnn_model = RNN(embeddings, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Entraînement du modèle: 37 minutes chez moi => modèle chargeable dans la case suivante\n",
    "train_1to1(preprocessor, rnn_model, 50, raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_model(preprocessor, model,fichier): # pas de sauvegarde de l'optimiseur ici\n",
    "      \"\"\" sauvegarde du modèle dans fichier \"\"\"\n",
    "      state = {'model_state': model.state_dict(), 'preprocessor': preprocessor}\n",
    "      torch.save(state,fichier) # pas besoin de passer par pickle\n",
    " \n",
    "def load_model_RNN(fichier):\n",
    "      \"\"\" Si le fichier existe, on charge le modèle  \"\"\"\n",
    "      if os.path.isfile(fichier):\n",
    "          state = torch.load(fichier)\n",
    "          preprocessor = state['preprocessor']\n",
    "          embeddings = preprocessor.embeddings(50)\n",
    "          model = RNN(embeddings, 100)\n",
    "          model.load_state_dict(state['model_state'])\n",
    "          return preprocessor, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde\n",
    "# save_model(preprocessor, rnn_model, \"model/rnn-gen\")\n",
    "\n",
    "# chargement ATTENTION à ne pas écraser un modèle appris\n",
    "preprocessor,rnn_model  = load_model_RNN(\"model/rnn-gen\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de visualiser ce que génère le modèle, nous pouvons utiliser une méthode simple qui consiste à échantilloner à chaque étape le mot à générer, puis à conditionner en fonction de ce qui a été choisi :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(preprocessor: Preprocessor, model, start=\"it is\", maxlength=50):\n",
    "    \"\"\"Génère une suite de tokens en utilisant la distribution de probabilité du modèle\"\"\"\n",
    "\n",
    "    assert start, \"le début de la phrase doit être indiqué\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = preprocessor.process_batch([start]).to(device)\n",
    "        generated = []\n",
    "        y_t, s_t = model(x)\n",
    "        # for borné sur maxlength\n",
    "        #   tirage d'un mot selon les y\n",
    "        #   si le mot n'est pas eos\n",
    "        #       reappeler le modèle (il faut un .unsqueeze(0) sur le mot tiré)\n",
    "        \n",
    "        #  TODO \n",
    "        return start + \" \" + preprocessor.decode(generated)\n",
    "\n",
    "generate(preprocessor, rnn_model, start=\"i am curious: yellow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"alert-success\"> Exercice : GRU et LSTM </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez à la place de la cellule RNN usuelle un GRU (ou un LSTM, les GRUs sont un peu plus faciles à utiliser) et comparez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le code est identique... \n",
    "#  cellule nn.RNN => nn.LSTM\n",
    "\n",
    "#  TODO \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Entraînement du modèle\n",
    "train_1to1(preprocessor, lstm_model, 50, raw_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_LSTM(fichier):\n",
    "      \"\"\" Si le fichier existe, on charge le modèle  \"\"\"\n",
    "      if os.path.isfile(fichier):\n",
    "          state = torch.load(fichier)\n",
    "          preprocessor = state['preprocessor']\n",
    "          embeddings = preprocessor.embeddings(50)\n",
    "          model = LSTM(embeddings, 100)\n",
    "          model.load_state_dict(state['model_state'])\n",
    "          return preprocessor, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde\n",
    "# save_model(preprocessor, lstm_model, \"model/lstm-gen\")\n",
    "\n",
    "# chargement (attention à ne pas écraser !!!)\n",
    "preprocessor,lstm_model  = load_model_LSTM(\"model/lstm-gen\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(preprocessor, lstm_model, start='''\"i am curious: yellow\" is a risible and pretentious''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Améliorer la génération de texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lors de vos tentatives de génération, vous observerez que en prenant à chaque pas de temps l'argmax vous obtiendrez très peu souvent des phrases intelligibles (vous faîtes en fait une approximation gloutonne du maximum de vraisemblance). Même l'échantillonage dans la distribution inférée ne rend pas bien meilleur le résultat. La solution usuelle consiste à utiliser un *beam search* pour approximer l'argmax sur toute la séquence engendrée : le beam search consiste à conserver à tout moment $t$ un ensemble de $K$ séquences (et leur log-probabilité associée); à l'étape $t+1$, on génère pour chacune des  séquences  $s$ les $K$ symboles les plus probables étant donnée $s$. Puis on sélectionne les $K$ séquence de taille $t+1$ les plus probables (et on ré-itère).\n",
    "\n",
    "La qualité des séquences générées peut encore être améliorée en utilisant des techniques d'échantillonage telle que le Nucleus Sampling qui consiste à définir la probabilité de génération en ne considérant que les caractères les plus probables, en définissant un seuil $\\alpha$ (un hyperparamètre, ex. 0.95) qui permet de sélectionner seulement les sorties permettant de couvrir au mieux cette masse de probabilité. Formellement, si $I_\\alpha(p, s)$ est le plus petit ensemble de symboles tel que \n",
    "$$ \\sum_{I_\\alpha(p, s)} {p(y|s)}  \\ge \\alpha $$\n",
    "\n",
    "La probabilité **nucleus** est définie comme \n",
    "$$\n",
    "p_{\\textrm{nucleus } K}(y|s_t) =\n",
    "  1_{y \\in I_\\alpha(p, s_t)} \\times \\frac{p(y|s_t)}{\\sum_{y^\\prime\\in I_\\alpha(p, s_t)} p(y^\\prime|s_t)}\n",
    "$$\n",
    "\n",
    "Implémentez la génération beam-search avec nucleus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# representation de la chaine de départ\n",
    "\n",
    "def compute_h0(preprocessor, model, start):\n",
    "    with torch.no_grad():\n",
    "        res = \"\"\n",
    "        if start:\n",
    "            tids = preprocessor.string2idx(start)#[:-1]  # processing de la chaine start\n",
    "            x = torch.LongTensor(tids).to(device)       # construction du tenseur d'indice\n",
    "            output, hidden = model.forward(x.unsqueeze(1))  # traitement de la chaine\n",
    "            res = tids\n",
    "            # print(res)\n",
    "        else:\n",
    "            state = None\n",
    "    return [int(x) for x in res], output[-1], hidden    # recuperation de l'ensemble des états cachés\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, output, state =  compute_h0(preprocessor, rnn_model, \"i was happy\")\n",
    "print(preprocessor.decode(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_beam(preprocessor, model, k, start=\"\", maxlen=50):\n",
    "    with torch.no_grad():\n",
    "        res, output, state = compute_h0(preprocessor, model, start)\n",
    "        \n",
    "        # état, chaîne, log prob, length, eos\n",
    "        candidates = [ (output, state, res, 0, len(preprocessor.string2idx(start)), False) ]\n",
    "        \n",
    "        \n",
    "        for i in range(maxlen):\n",
    "            new_candidates = []\n",
    "            for output, state, res, logp, length, is_eos in candidates:\n",
    "                if is_eos:\n",
    "                    new_candidates.append((None, None, res, logp, length, None))\n",
    "                else:\n",
    "                    values, indices = torch.log_softmax(output, dim=1).topk(k)\n",
    "                    for logp_c, c in zip(values[0], indices[0]):\n",
    "                        if c !=0:\n",
    "                            new_candidates.append((output, state, res, logp + logp_c, length+1, c))\n",
    "\n",
    "            candidates = []\n",
    "            for output, state, res, logp, length, c in sorted(new_candidates, key=lambda x: -x[3])[:k]:\n",
    "                if c is None:\n",
    "                    candidates.append((None, None, res, logp, length, True))\n",
    "                else:\n",
    "                    new_out, new_state = model.forward(c.view(1,1), state)\n",
    "                    candidates.append((new_out[-1], new_state, res + [int(c)], logp, length, c == preprocessor.eos_index))\n",
    "\n",
    "    return [c[2] for c in candidates]\n",
    "\n",
    "\n",
    "display(HTML(\"<div><b>Avec RNN</b></div>\"))\n",
    "beams = generate_beam(preprocessor, rnn_model, 15, \"i was happy\")\n",
    "print(preprocessor.decode(beams[0]))\n",
    "\n",
    "display(HTML(\"<div><b>Avec LSTM</b></div>\"))\n",
    "beams = generate_beam(preprocessor, lstm_model, 15, \"i was disappointed\")\n",
    "for txt in beams:\n",
    "    print(preprocessor.decode(txt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  TODO )\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('torch-nightly')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc-showtags": true,
  "vscode": {
   "interpreter": {
    "hash": "0539fa28f95d3c31ab10ef7831d62bf0e47751f442c28c9f64a267247464e7da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
