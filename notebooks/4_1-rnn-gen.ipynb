{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 : RNN et attention pour le texte \n",
    "\n",
    "\n",
    "Vincent Guigue, inspiré de\n",
    "Nicolas Baskiotis (nicolas.baskiotis@isir.upmc.fr) Benjamin Piwowarski (benjamin@piwowarski.fr)  -- MLIA/LIP6, ISIR, Sorbonne Université"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La représentation moyenne étudiée au module précédent trouve vite ses limites lorsque l'on s'attaque à des tâches plus complexes (traduction, résumé automatique, ...) où ce qui nous intéresse est à une fine granularité dans le texte. Ce module explore les architectures récurrentes qui permettent de bien mieux traiter le texte. \n",
    "\n",
    "Nous allons étudier la *génération libre de texte* en utilisant `XSum`, un jeu de données de résumé automatique.\n",
    "\n",
    "La génération de séquence consiste  à produire  une séquence de symboles discrets à partir d'une séquence en entrée. L'objectif est donc de décoder à partir de l'état caché une distribution  de probabilités multinomiale sur les symboles à engendrer. Il faut donc que la  dimension de sortie du RNN (du décodeur) soit égale au nombre de symboles considérés. Il faut par ailleurs utiliser un *softmax* pour obtenir une distribution à partir du décodage de l'état caché. Le coût cross-entropie est adapté pour apprendre cette distribution.\n",
    "\n",
    "Une fois le réseau appris, la génération se fait (soit à partir d'un début de séquence, soit à partir d'un état initial vide) en choisissant le symbole le plus probable dans la distribution multinomiale décodée à chaque pas de temps. Ce symbole est ensuite considéré comme entrée au pas de temps suivant et la génération se poursuit itérativement. Une autre possibilité est d'échantillonner suivant la multinomiale pour obtenir plusieurs échantillons. La génération se poursuit jusqu'à produire le token *\\<eos\\>*.\n",
    "\n",
    "La supervision se fait à chaque pas de temps (contrairement à la classification où la supervision se fait au dernier pas de temps) : c'est un exemple de réseau many-to-many. On veut inciter le réseau à produire à l'instant $t+1$ le token suivant de notre séquence. Ainsi à tout pas de temps $t$ on connaît la supervision. Le gradient du coût à propager est la somme de tous les coûts à chaque instant. Il ne faut pas oublier cependant à ne pas prendre en compte les tokens de padding dans le calcul du coût."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from typing import Iterable, List, Tuple\n",
    "\n",
    "import shutil\n",
    "import torchtext\n",
    "assert version.Version(torchtext.__version__) >= version.Version(\"0.9.0\")\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab, FastText\n",
    "from collections import Counter\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import sentencepiece as spm\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "\n",
    "cachepath = os.path.expanduser('~/.local/data')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# outils avancés de gestion des chemins\n",
    "BASEPATH = Path(\"xp/generation\")\n",
    "TB_PATH =  BASEPATH / \"logs\"\n",
    "TB_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# usage externe de tensorboard: (1) lancer la commande dans une console; (2) copier-coller l'URL dans un navigateur\n",
    "display(HTML(\"<h2>Informations</h2><div>Pour visualiser les logs, tapez la commande : </div>\"))\n",
    "print(f\"tensorboard --logdir {Path(TB_PATH).absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentencepiece\n",
    "# !pip install datasets\n",
    "# !pip install fsspec==2021.8.1\n",
    "# !pip uninstall huggingface-hub\n",
    "!pip install rouge-score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Chargement et pré-traitement & exploration des données (XSum)\n",
    "\n",
    "Ci-dessous les fonctions qui permettent de charger les données et de les préparer (en lots/batchs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datasets import load_dataset, load_metric\n",
    "import datasets as ds\n",
    "\n",
    "raw_datasets = ds.load_dataset(\"xsum\", split={\"train\": \"train[:10%]\", \"validation\": \"validation[:5%]\", \"test\": \"validation[5%:]\"})\n",
    "metric = ds.load_metric(\"rouge\")\n",
    "\n",
    "# Affiche les champs \n",
    "raw_datasets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "### Extrait du jeu de données\n",
    "\n",
    "Voici un extrait aléatoire du jeu de données, avec un document (à résumé) et le résumé attendu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13"
   },
   "outputs": [],
   "source": [
    "print(raw_datasets[\"train\"][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réduction du vocabulaire"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les réseaux récurrents sont lourds à mettre en oeuvre : pour chaque mini-batch il faut itérer sur la longueur des séquences. Il est donc judicieux de réduire autant que possible les dimensionalités à traiter.\n",
    " \n",
    "Le pré-traitement des textes repose sur une étape de segmentation où le texte est découpé en unités linguistiques. Pendant longtemps le niveau choisi était le mot (= chaîne alphanumérique entourée d'espace); depuis quelques années, des alternatives ont été (ré)explorées avec les nouveaux modèles neuronaux. \n",
    "\n",
    "Une des segmentations les plus efficaces à l'heure actuelle est le découpage en n-grammes variables (**subword units**) popularisé par le Byte-Pair Encoding (BPE) en 2016. Ces segmentations ont l'avantage d'avoir un vocabulaire de taille fixe qui couvre au mieux le jeu de données, et permet d'éviter le problème des mots inconnus.\n",
    "\n",
    "Par exemple, *You shoulda got David Carr of Third Day to do it* sera segmenté en ```\"\\_You\", \"\\_should\", \"a\", \"\\_got\", \"\\_D\", \"av\", \"id\", \"\\_C\", \"ar\", \"r\", \"\\_of\", \"\\_Th\", \"ir\", \"d\", \"\\_Day\", \"\\_to\", \"\\_do\", \"\\_it\"```\n",
    "\n",
    "où les séquences fréquentes (ex. You, should) sont extraites directement alors que des séquences moins fréquentes (ex. David, Carr) sont segmentées en plusieurs parties.\n",
    "\n",
    "La librairie [sentencepiece](https://github.com/google/sentencepiece/blob/master/python/README.md)</a> permet une telle segmentation. Les tokens Unknown *\\<unk\\>*, *BOS* (begin of sequence, *\\<s\\>*) and *EOS* (end of sequence, *\\</s\\>*) sont prédéfinis, mais vous pouvez en ajouter d'autres avec **user_defined_symbols**.\n",
    "    \n",
    "Nous allons également considéré qu'un sous-échantillon des exemples pour rendre plus rapide l'apprentissage.\n",
    "\n",
    "Ne pas oublier de préfixer chaque phrase par le token *\\<bos\\>* et le suffixé par *\\<eos\\>*, cela nous servira dans des tâches futures."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La suite est classique : remarquez dans **pad_sequence** que cette fois on ne passe pas l'argument **batch_first=True** pour garder les conventions usuelles RNNs : la première dimension du tenseur est la longueur, la deuxième la taille du batch, la troisième la dimension d'embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def process_batch(self, batch: List[str]):            \n",
    "        return pad_sequence([self.string2idx(text) for text in batch])\n",
    "\n",
    "class SentencePiecePreprocessor(Preprocessor):\n",
    "    \"\"\"Tokenizer Sentence Piece\"\"\"\n",
    "    def embeddings(self, dimension: int):\n",
    "        return nn.Embedding(self.vocab_size, dimension, padding_idx=self.pad_index)\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return self.spm_tokenizer.vocab_size()\n",
    "\n",
    "    def __init__(self, name: str, sentences: Iterable[str], vocab_size=1000, force=False):\n",
    "        modelpath = BASEPATH / f\"{name}-{vocab_size}.model\"\n",
    "        if not modelpath.exists() or force:\n",
    "            print(f\"Entraînement de SPM, sortie {BASEPATH}/{name}-{vocab_size}\", flush=True)\n",
    "            spm.SentencePieceTrainer.train(\n",
    "                sentence_iterator=iter(sentences), \n",
    "                model_prefix=f\"{BASEPATH}/{name}-{vocab_size}\", \n",
    "                vocab_size=vocab_size,\n",
    "                pad_id=0,                \n",
    "                unk_id=1,\n",
    "                bos_id=2,\n",
    "                eos_id=3\n",
    "            )\n",
    "            \n",
    "        self.spm_tokenizer = spm.SentencePieceProcessor()\n",
    "        self.spm_tokenizer.load(str(modelpath))\n",
    "\n",
    "        self.pad_index = self.spm_tokenizer.pad_id()\n",
    "        self.eos_index = self.spm_tokenizer.eos_id()\n",
    "        self.bos_index = self.spm_tokenizer.bos_id()\n",
    "        self.oov_index = None\n",
    "\n",
    "        self.spm_tokenizer.SetEncodeExtraOptions(\"bos:eos\")\n",
    "\n",
    "    def string2idx(self, s: str) -> torch.Tensor:\n",
    "        return torch.tensor(self.spm_tokenizer.EncodeAsIds(s))\n",
    "\n",
    "    def tokenizer(self, x):\n",
    "        \"\"\"Segmentation du texte en sous-mots\"\"\"\n",
    "        return self.spm_tokenizer.encode_as_pieces(x)\n",
    "\n",
    "    def id2token(self, ix):\n",
    "        return self.spm_tokenizer.IdToPiece(ix)\n",
    "\n",
    "    def decode(self, ids: List[int]):\n",
    "        return self.spm_tokenizer.Decode(ids)\n",
    "\n",
    "\n",
    "# Si vous voulez essayer avec des mots\n",
    "# preprocessor = FastTextProcessor()\n",
    "\n",
    "preprocessor = SentencePiecePreprocessor(\"spm\", (row[\"summary\"] for row in raw_datasets[\"train\"]), force=False)\n",
    "print(f\"Vocabulary size: {preprocessor.vocab_size}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On essaie notre nouveau tokenizer sur \"hello world\" pour voir ce qui se passe. Notez :\n",
    "\n",
    "1. Le découpage en sous-mots\n",
    "1. La présence de tokens spéciaux (début `<s>` et fin de séquence `</s>`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemple de décomposition avec BPE\n",
    "preprocessor.tokenizer(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la fonction que nous allons utiliser dans la procédure d'apprentissage:\n",
    "out = preprocessor.process_batch([\"hello world\"]) # => tenseur d'indices présenté en colonne\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation inverse (index => token)\n",
    "# attention (toujours aux dimensions) : si on oublie le squeeze => list de token\n",
    "\n",
    "print(preprocessor.decode(out.tolist()))\n",
    "print(preprocessor.decode(out.squeeze().tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reflexion sur le padding\n",
    "xnum = preprocessor.process_batch([\"hello world\", \"hello word plus one word\"])\n",
    "\n",
    "print(\"Dimension de x :\", xnum.size()) # T x batch_size x dim_input\n",
    "print(xnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération automatique d'une lookup table (tableau d'embeddings):\n",
    "embeddings = preprocessor.embeddings(50) # taille vaocabulaire x dim = 50\n",
    "\n",
    "print(\"taille de la table:\", embeddings.num_embeddings, \"x\", embeddings.embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evidemment, tout est cohérent: on récupère facilement la séquence d'embedding:\n",
    "x = embeddings(preprocessor.process_batch([\"hello world\"]))\n",
    "\n",
    "print(\"Dimension de x :\", x.size()) # T x batch_size x dim_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verification sur la nature de la représentation du mot de padding\n",
    "\n",
    "x = embeddings(preprocessor.process_batch([\"hello world\", \"hello word plus one word\"]))\n",
    "\n",
    "print(\"Dimension de x :\", x.size()) # T x batch_size x dim_input\n",
    "\n",
    "print(\"mot <pad> :\", x[5,0,:].data) # 5e mot du doc 0 (toutes les dimensions de la représentation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "et maintenant, on traite notre jeu de données en entier avec la fonction `map` de datasets et une fonction de pré-traitement `preprocess_fn`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Rappel de cours sur le module RNN\n",
    "\n",
    "* Dimensions & nature des entrées\n",
    "* Dimensions & nature des sorties\n",
    "\n",
    "Note: le traitement des séquences étant complexe, le module est plus imbriqué que les fully connected + cnn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.RNN(input_size=100, hidden_size=50, \n",
    "                     num_layers=1,\n",
    "                     nonlinearity='tanh', bias=True)\n",
    "\n",
    "# data (x_1,...,x_T), batch, dimension des entrées\n",
    "#   dim1 : Seq. length T=23 \n",
    "#   dim2 : batch of size 10\n",
    "#   dim3 : Input x_t \\in R^n dimension 100\n",
    "seq_x = torch.rand(23, 10, 100) # simulation d'un batch\n",
    "\n",
    "# For classification purpose, we use last_h\n",
    "seq_h, last_h = model(seq_x)\n",
    "\n",
    "print(\"seq_h : \",seq_h.size())\n",
    "print(\"last_h: \", last_h.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. RNN simple \n",
    "\n",
    "Procédure d'apprentissage\n",
    "\n",
    "Le code est très classique, sauf la loss qu'il faut regarder en détail.\n",
    "\n",
    "1. On va prendre une décision à chaque pas de temps\n",
    "1. Il y aura une supervision à chaque pas de temps\n",
    "1. On va rétro-propager $T-1$ coûts pour une séquence\n",
    "1. Reflexion spécifique pour les mots de padding\n",
    "\n",
    "<img src=\"media/teacher.png\">\n",
    "\n",
    "Il faut aussi comprendre le code oringial:\n",
    "```\n",
    "loss = nn.CrossEntropyLoss(ignore_index=preprocessor.pad_index)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCHSIZE = 128\n",
    "TEST_BATCHSIZE = 512\n",
    "\n",
    "def train_1to1(preprocessor, model, epochs, raw_datasets):\n",
    "    \"\"\"Fonction d'entraînement (teacher forcing)\"\"\"\n",
    "    print(f\"Training {model.name}\")\n",
    "    \n",
    "    # On nettoie le rep. de log\n",
    "    shutil.rmtree(f\"{TB_PATH}/{model.name}\", ignore_errors = True)\n",
    "    writer = SummaryWriter(f\"{TB_PATH}/{model.name}\")\n",
    "    \n",
    "    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_loader = DataLoader(raw_datasets[\"train\"], TRAIN_BATCHSIZE, shuffle=True)\n",
    "    test_loader = DataLoader(raw_datasets[\"test\"], TEST_BATCHSIZE, shuffle=False)\n",
    "    loss = nn.CrossEntropyLoss(ignore_index=preprocessor.pad_index)  # classification avec une classe qui ne retourne pas d'erreur\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        cumloss, count =  0, 0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optim.zero_grad()\n",
    "            x = preprocessor.process_batch(batch[\"summary\"]).to(device)\n",
    "        \n",
    "            # Mode \"Teacher forcing\"\n",
    "            x, y = x[:-1], x[1:] # cf schéma ci-dessus\n",
    "\n",
    "            yhat, hidden = model.forward(x)\n",
    "            l = loss(yhat.view(-1,yhat.size(2)), y.view(-1))\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l*len(x)\n",
    "            count += len(x)\n",
    "        writer.add_scalar('loss/train', cumloss/count, epoch)\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                cumloss, count = 0, 0\n",
    "                for x in test_loader:\n",
    "                    x = preprocessor.process_batch(batch[\"summary\"])\n",
    "                    x, y = x[:-1].to(device), x[1:].to(device)\n",
    "                    yhat, hidden = model(x)\n",
    "                    cumloss += loss(yhat.view(-1,yhat.size(2)),y.view(-1))*len(x)\n",
    "                    count += len(x)\n",
    "                writer.add_scalar(f'loss/test',cumloss/count,epoch)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le RNN utilisé pour la génération est composé :\n",
    "* d'une [couche d'embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) pour la représentation des tokens\n",
    "* d'une <a href=https://pytorch.org/docs/stable/generated/torch.nn.RNN.html> cellule RNN</a>\n",
    "* et d'un *décodeur*, chargé de décoder l'état latent du RNN vers la classe *positive* ou *négative*, sous la forme d'un réseau linéaire.\n",
    "\n",
    "Le **forward** d'un module **RNN** retourne deux tenseurs, le premier correspond au tenseur **output** de  taille `Longueur x Batch x Taille du vocabulaire`, le deuxième au tenseur **hidden** qui correspond juste au dernier état caché. Ce tenseur du dernier état caché est très utile lorsque l'on veut poursuivre l'inférence à partir de l'état de sortie pour initialiser l'état caché du réseau (cf fonction `generate`).\n",
    "\n",
    "Dans la cellule suivante, la boucle d'apprentissage (teacher forcing) est donnée. Vous pouvez modifier le code, si vous le souhaitez, pour utiliser des techniques d'apprentissage plus évoluées (utilisation de la sortie du RNN plutôt que de la vérité de terrain).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Donnez l'implémentation du module RNN\n",
    "\n",
    "Il ne vous reste plus qu'à définir l'initialisation des modules (`__init__`) ainsi que leur utilisation (`forward`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.name = f\"rnn-{embeddings.embedding_dim}-{hidden_dim}\"\n",
    "        self.embeddings = embeddings # stockage des embeddings (fournis par le preprocessor)\n",
    "\n",
    "        # 2 lignes à compléter = 2 attributs à créer\n",
    "        #    => Les nommer self.rnn & self.fc pour la cohérence avec les modèles chargeables\n",
    "        #\n",
    "        # definir la partie recurrente dim_embedding => dim_h\n",
    "        # prediction de char: le predicteur doit revenir dans l'espace des embeddings\n",
    "        #  TODO \n",
    "        \n",
    "    \n",
    "    def forward(self, x, h_0=None): \n",
    "        # x = numero des mots\n",
    "        # h_0 = état caché de la séquence à poursuivre (si elle existe)\n",
    "        #   => Récupérer les embedding\n",
    "        #   => passer dans le RNN\n",
    "        #   => passer TOUS les états cachés du RNN dans le fully-connected (cf définition de la loss ci-dessus)\n",
    "        # La sortie contiendra les scores + le dernier état caché (pour pouvoir itérer la génération, cf plus tard)\n",
    "        #  TODO \n",
    "        # Attention de bien prendre en compte h_0 dans l'appel du RRN\n",
    "        \n",
    "\n",
    "# On récupère des embeddings pour le modèle (dimension 50)\n",
    "embeddings = preprocessor.embeddings(50)\n",
    "\n",
    "# Création du RNN avec 100 états cachés\n",
    "rnn_model = RNN(embeddings, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Vérifier que vorte code est fonctionnel: l'apprentissage se lance correctement\n",
    "# 2. Récupération du modèle entrainé pour gagner du temps\n",
    "\n",
    "# Entraînement du modèle: 37 minutes chez moi => modèle chargeable dans la case suivante\n",
    "train_1to1(preprocessor, rnn_model, 50, raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_model(preprocessor, model,fichier): # pas de sauvegarde de l'optimiseur ici\n",
    "      \"\"\" sauvegarde du modèle dans fichier \"\"\"\n",
    "      state = {'model_state': model.state_dict(), 'preprocessor': preprocessor}\n",
    "      torch.save(state,fichier) # pas besoin de passer par pickle\n",
    " \n",
    "def load_model_RNN(fichier):\n",
    "      \"\"\" Si le fichier existe, on charge le modèle  \"\"\"\n",
    "      if os.path.isfile(fichier):\n",
    "          state = torch.load(fichier)\n",
    "          preprocessor = state['preprocessor']\n",
    "          embeddings = preprocessor.embeddings(50)\n",
    "          model = RNN(embeddings, 100)\n",
    "          model.load_state_dict(state['model_state'])\n",
    "          return preprocessor, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde\n",
    "# save_model(preprocessor, rnn_model, \"model/rnn-gen\")\n",
    "\n",
    "# chargement ATTENTION à ne pas écraser un modèle appris\n",
    "preprocessor,rnn_model  = load_model_RNN(\"model/rnn-gen\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnn_model(torch.tensor([2])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Génération de token\n",
    "\n",
    "Afin de visualiser ce que génère le modèle, nous pouvons utiliser une méthode simple qui consiste à échantilloner à chaque étape le mot à générer, puis à conditionner en fonction de ce qui a été choisi :\n",
    "\n",
    "1. Traiter la séquence de départ et récupérer TOUS les états cachés `s_t` [vérifier les dimensions des tenseurs]\n",
    "1. Boucle (bornée sur la longueur max)\n",
    "    1. Récupérer le token généré `w`\n",
    "        * soit argmax\n",
    "        * soit tirage selon la distribution multinomiale des $y$ (avantage: on peut tirer différentes solutions)\n",
    "    1. S'il est = fin de phrase (`preprocessor.eos_index`) => break\n",
    "    1. Ajouter `w` dans generated (=liste d'entier)\n",
    "    1. Prédire le prochain `w` à partir de `w` + `s_t` (cf h0 dans le forward du réseau rnn)\n",
    "\n",
    "\n",
    "Cet exercice comporte des difficultés au niveau des structures de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# micro-tuto : tirage selon une distribution de score (positif & negatif = sortie du linear)\n",
    "# 1. score => distrib (softmax / logits)\n",
    "# 2. Tirage\n",
    "\n",
    "score = torch.tensor([1.,2.,5.]) \n",
    "score = torch.tensor([- 0.1, 0.5, 0.9])\n",
    "for i in range (10):\n",
    "    print(\"tirage : \",torch.distributions.categorical.Categorical(logits=score).sample().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(preprocessor: Preprocessor, model, start=\"it is\", maxlength=50):\n",
    "    \"\"\"Génère une suite de tokens en utilisant la distribution de probabilité du modèle\"\"\"\n",
    "\n",
    "    assert start, \"le début de la phrase doit être indiqué\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = preprocessor.process_batch([start]).to(device)\n",
    "        generated = []\n",
    "        y_t, s_t = model(x)\n",
    "        # for borné sur maxlength\n",
    "        #   tirage d'un mot selon les y\n",
    "        #   si le mot n'est pas eos\n",
    "        #       reappeler le modèle (il faut un .unsqueeze(0) sur le mot tiré)\n",
    "        \n",
    "        #  TODO \n",
    "        # print(generated) # liste d'indices\n",
    "        return start + \" \" + preprocessor.decode(generated)\n",
    "\n",
    "generate(preprocessor, rnn_model, start=\"i am curious: yellow\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"alert-success\"> Exercice : GRU et LSTM </span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez à la place de la cellule RNN usuelle un GRU (ou un LSTM, les GRUs sont un peu plus faciles à utiliser) et comparez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le code est identique... \n",
    "#  cellule nn.RNN => nn.LSTM\n",
    "\n",
    "#  TODO \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Entraînement du modèle\n",
    "train_1to1(preprocessor, lstm_model, 50, raw_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_LSTM(fichier):\n",
    "      \"\"\" Si le fichier existe, on charge le modèle  \"\"\"\n",
    "      if os.path.isfile(fichier):\n",
    "          state = torch.load(fichier)\n",
    "          preprocessor = state['preprocessor']\n",
    "          embeddings = preprocessor.embeddings(50)\n",
    "          model = LSTM(embeddings, 100)\n",
    "          model.load_state_dict(state['model_state'])\n",
    "          return preprocessor, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde\n",
    "# save_model(preprocessor, lstm_model, \"model/lstm-gen\")\n",
    "\n",
    "# chargement (attention à ne pas écraser !!!)\n",
    "preprocessor,lstm_model  = load_model_LSTM(\"model/lstm-gen\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le code doit être fonctionnel pour la nouvelle architecture\n",
    "generate(preprocessor, lstm_model, start='''\"i am curious: yellow\" is a risible and pretentious''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Améliorer la génération de texte"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lors de vos tentatives de génération, vous observerez que en prenant à chaque pas de temps l'argmax vous obtiendrez très peu souvent des phrases intelligibles (vous faîtes en fait une approximation gloutonne du maximum de vraisemblance). Même l'échantillonage dans la distribution inférée ne rend pas bien meilleur le résultat. La solution usuelle consiste à utiliser un *beam search* pour approximer l'argmax sur toute la séquence engendrée : le beam search consiste à conserver à tout moment $t$ un ensemble de $K$ séquences (et leur log-probabilité associée); à l'étape $t+1$, on génère pour chacune des  séquences  $s$ les $K$ symboles les plus probables étant donnée $s$. Puis on sélectionne les $K$ séquence de taille $t+1$ les plus probables (et on ré-itère).\n",
    "\n",
    "La qualité des séquences générées peut encore être améliorée en utilisant des techniques d'échantillonage telle que le Nucleus Sampling qui consiste à définir la probabilité de génération en ne considérant que les caractères les plus probables, en définissant un seuil $\\alpha$ (un hyperparamètre, ex. 0.95) qui permet de sélectionner seulement les sorties permettant de couvrir au mieux cette masse de probabilité. Formellement, si $I_\\alpha(p, s)$ est le plus petit ensemble de symboles tel que \n",
    "$$ \\sum_{I_\\alpha(p, s)} {p(y|s)}  \\ge \\alpha $$\n",
    "\n",
    "La probabilité **nucleus** est définie comme \n",
    "$$\n",
    "p_{\\textrm{nucleus } K}(y|s_t) =\n",
    "  1_{y \\in I_\\alpha(p, s_t)} \\times \\frac{p(y|s_t)}{\\sum_{y^\\prime\\in I_\\alpha(p, s_t)} p(y^\\prime|s_t)}\n",
    "$$\n",
    "\n",
    "Implémentez la génération beam-search avec nucleus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# representation de la chaine de départ\n",
    "# preprocessing => model \n",
    "\n",
    "def compute_h0(preprocessor, model, start):\n",
    "    with torch.no_grad():\n",
    "        res = \"\"\n",
    "        if start:\n",
    "            tids = preprocessor.string2idx(start)#[:-1]  # processing de la chaine start\n",
    "            x = torch.LongTensor(tids).to(device)       # construction du tenseur d'indice\n",
    "            output, hidden = model.forward(x.unsqueeze(1))  # traitement de la chaine\n",
    "            res = tids\n",
    "            # print(res)\n",
    "        else:\n",
    "            state = None\n",
    "    return [int(x) for x in res], output[-1], hidden    # cf boite suivante\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, output, state =  compute_h0(preprocessor, rnn_model, \"i was happy\")\n",
    "\n",
    "print(res,'=',preprocessor.decode(res))\n",
    "print(\"p(w|h) :\",output.size())\n",
    "print(\"h :\",state.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_beam(preprocessor, model, k, start=\"\", maxlen=50):\n",
    "    with torch.no_grad():\n",
    "        res, output, state = compute_h0(preprocessor, model, start)\n",
    "        \n",
    "        # p(w|h)     h       chaîne      log prob (\\sum_t log p(w_t))    length      eos\n",
    "        candidates = [ (output, state, res, 0, len(preprocessor.string2idx(start)), False) ]\n",
    "        \n",
    "        for i in range(maxlen):\n",
    "            new_candidates = []\n",
    "            # filtrage des branches à allonger (new_candidates)\n",
    "            for output, state, res, logp, length, is_eos in candidates:\n",
    "                if is_eos:\n",
    "                    new_candidates.append((None, None, res, logp, length, None))\n",
    "                else:\n",
    "                    values, indices = torch.log_softmax(output, dim=1).topk(k) # récupération des k meilleures propositions\n",
    "                    for logp_c, c in zip(values[0], indices[0]):\n",
    "                        if c !=0:\n",
    "                            new_candidates.append((output, state, res, logp + logp_c, length+1, c))\n",
    "            \n",
    "            # pour les branches à allonger, ajouter un mots\n",
    "            candidates = []\n",
    "            for output, state, res, logp, length, c in sorted(new_candidates, key=lambda x: -x[3])[:k]: # ordonner selon la vraisemblance\n",
    "                if c is None:\n",
    "                    candidates.append((None, None, res, logp, length, True))\n",
    "                else:\n",
    "                    new_out, new_state = model.forward(c.view(1,1), state) # ajouter un mot\n",
    "                    candidates.append((new_out[-1], new_state, res + [int(c)], logp, length, c == preprocessor.eos_index)) # MAJ candidats\n",
    "\n",
    "    return [c[2] for c in candidates]\n",
    "\n",
    "\n",
    "display(HTML(\"<div><b>Avec RNN</b></div>\"))\n",
    "beams = generate_beam(preprocessor, rnn_model, 15, \"i was happy\")\n",
    "print(preprocessor.decode(beams[0]))\n",
    "\n",
    "display(HTML(\"<div><b>Avec LSTM</b></div>\"))\n",
    "beams = generate_beam(preprocessor, lstm_model, 15, \"i was disappointed\")\n",
    "for txt in beams:\n",
    "    print(preprocessor.decode(txt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  TODO )\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-showtags": true,
  "vscode": {
   "interpreter": {
    "hash": "902a52bcf4503a473db011f1937bdfe17613b08622219712e0110e48c4958c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
