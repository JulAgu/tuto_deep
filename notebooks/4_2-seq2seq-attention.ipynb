{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G√©n√©ration condtionn√©e (Seq2Seq) avec des RNNs et de l'attention\n",
    "\n",
    "Dans le TP pr√©c√©dent, nous avons utilis√© des RNNs pour g√©n√©rer du texte \"libre\" - ou bien conditionn√© par\n",
    "le d√©but de la s√©quence. Pour certaines t√¢ches, comme par exemple la traduction ou la cr√©ation\n",
    "de l√©gendes pour les images, il peut √™tre int√©ressant de traiter de mani√®re\n",
    "diff√©rente la repr√©sentation des donn√©es en entr√©es et en sortie.\n",
    "\n",
    "De plus, afin d'am√©liorer les performance des mod√®les, les RNNs peuvent utiliser une \"m√©moire\" - dans\n",
    "notre cas, il s'agit du texte en entr√©e. Cette id√©e est reprise dans les transformers que nous \n",
    "verrons dans le module suivant.\n",
    "\n",
    "Dans cette partie, nous allons introduire deux nouveaut√©s par rapport aux RNNs du TP pr√©c√©dent :\n",
    "\n",
    "1. Nous allons utiliser un encodeur et un d√©codeur (seq2seq) avec des param√®tres distincts\n",
    "1. Nous allons utiliser un m√©canisme d'attention\n",
    "\n",
    "Les prochaines cellules permettent de charger et pr√©parer les donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir /Users/vguigue/Documents/Cours/Agro-IODAA/deep/notebooks/xp/seq2seq/logs\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "from typing import Tuple, Any, List, Union\n",
    "import shutil\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from tqdm.autonotebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "cachepath = os.path.expanduser('~/.local/data')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BASEPATH = Path(\"xp/seq2seq\")\n",
    "TB_PATH =  BASEPATH / \"logs\"\n",
    "TB_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"tensorboard --logdir {Path(TB_PATH).absolute()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons utiliser le m√™me jeu de donn√©es que dans le carnet pr√©c√©dent, mais en utilisant cette fois-ci les deux textes (document et r√©sum√©)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xsum (/Users/vguigue/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 92.17it/s]\n",
      "/var/folders/_h/xz4nr0h53dj3x3tygxjnzl540000gn/T/ipykernel_75062/62614406.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "# On prend juste 10% de la validation pour aller plus vite\n",
    "raw_datasets = load_dataset(\"xsum\", split={\"train\": \"train[:10%]\", \"validation\": \"validation[:5%]\", \"test\": \"validation[5%:]\"})\n",
    "\n",
    "# Dans le cadre du r√©sum√©, nous allons utiliser la m√©trique \"rouge\"\n",
    "rouge = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculates average rouge scores for a list of hypotheses and references\n",
      "Args:\n",
      "    predictions: list of predictions to score. Each prediction\n",
      "        should be a string with tokens separated by spaces.\n",
      "    references: list of reference for each prediction. Each\n",
      "        reference should be a string with tokens separated by spaces.\n",
      "    rouge_types: A list of rouge types to calculate.\n",
      "        Valid names:\n",
      "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
      "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
      "        `\"rougeLSum\"`: rougeLsum splits text using `\"\n",
      "\"`.\n",
      "        See details in https://github.com/huggingface/datasets/issues/617\n",
      "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
      "    use_aggregator: Return aggregates if this is set to True\n",
      "Returns:\n",
      "    rouge1: rouge_1 (precision, recall, f1),\n",
      "    rouge2: rouge_2 (precision, recall, f1),\n",
      "    rougeL: rouge_l (precision, recall, f1),\n",
      "    rougeLsum: rouge_lsum (precision, recall, f1)\n",
      "Examples:\n",
      "\n",
      "    >>> rouge = datasets.load_metric('rouge')\n",
      "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
      "    >>> references = [\"hello there\", \"general kenobi\"]\n",
      "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
      "    >>> print(list(results.keys()))\n",
      "    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
      "    >>> print(results[\"rouge1\"])\n",
      "    AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))\n",
      "    >>> print(results[\"rouge1\"].mid.fmeasure)\n",
      "    1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rouge.inputs_description)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plut√¥t que d'utiliser un vocabulaire entra√Æn√© sur les textes en apprentissage, nous allons utiliser ici un vocabulaire plus\n",
    "large qui a √©t√© utilis√© pour BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_h/xz4nr0h53dj3x3tygxjnzl540000gn/T/ipykernel_75062/1997467910.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizerFast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<bos>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<eos>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', bos_token=\"<bos>\", eos_token=\"<eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [\"<bos> This is the first document <eos>\", \"<bos> followed by the next one <eos>\", \"<bos> and the final text is here <eos>\"]\n",
    "r = tokenizer(batch,  truncation=True, add_special_tokens=False, return_token_type_ids=False, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "print(r)\n",
    "\n",
    "[\" \".join(tokenizer.convert_ids_to_tokens(row)) for row in r[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdata(batch, what: str, device):\n",
    "    \"\"\"Fonction utilitaire pour r√©duire la taille des donn√©es en fonction du batch\"\"\"\n",
    "\n",
    "    r = tokenizer([f\"<bos> {t} <eos>\" for t in batch[what]],  truncation=True, add_special_tokens=False, return_token_type_ids=False, padding=True, return_tensors=\"pt\", max_length=512)\n",
    "    # Renvoie dans le format RNN (temps en premier)\n",
    "    return (r[\"input_ids\"].T).to(device).contiguous()\n",
    "\n",
    "# Exemple\n",
    "loader = DataLoader(raw_datasets[\"train\"], batch_size=2)\n",
    "input_ids = getdata(next(iter(loader)), \"summary\", device)\n",
    "input_ids, input_ids.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"background: green; padding: 3px; color: white\">Exercice 1 : impl√©menter un seq2seq</span>\n",
    "\n",
    "La cellule suivante permet de d√©finir:\n",
    "\n",
    "- `RNNBase` qui est le prototype qui sera utilis√© par tous vos RNNs (encodeurs et d√©codeurs)\n",
    "- `Seq2Seq` qui est un mod√®le qui permet de regrouper encodeur, d√©codeur et classifieur (logits de la distribution multinomiale sur les tokens)\n",
    "- `train_seq2seq` qui permet d'apprendre un mod√®le `Seq2Seq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNNBase(nn.Module):\n",
    "    \"\"\"Cette classe sert de base pour tous vos mod√®les r√©currents\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.LongTensor, h_0=None, *, encoder_outputs=None, encoder_embeddings=None) -> Tuple[nn.Module, nn.Module, Any]:\n",
    "        \"\"\"M√©thode principale pour les r√©seaux r√©currents\n",
    "\n",
    "        Les param√®tres `encoder_*` serviront pour l'exercice 2\n",
    "\n",
    "        Args:\n",
    "            x (torch.LongTensor): Un tenseur contenant un batch de s√©quences sous forme d'ID de tokens (temps x batch) \n",
    "            h_0 (Any, optional): √âtat initial √† utiliser.\n",
    "            encoder_outputs (torch.Tensor, optional): Les sorties de l'encodeur\n",
    "            encoder_embeddings (torch.Tensor, optional): Les entr√©es de l'encodeur\n",
    "\n",
    "        Returns:\n",
    "            Tuple[nn.Module, nn.Module, Any]: Renvoie un tuple (embeddings, sorties du RNN, √©tat final)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"Mod√®le Seq2Seq g√©n√©rique\"\"\"\n",
    "\n",
    "    def __init__(self, name: str, encoder: nn.Module, decoder: nn.Module, classifier: nn.Module):\n",
    "        \"\"\"Initialise le mod√®le seq2seq\n",
    "\n",
    "        Args:\n",
    "            name (str): Le nom du mod√®le (pour tensorboard)\n",
    "            encoder (nn.Module): Un RNN qui encode\n",
    "            decoder (nn.Module): Un RNN qui d√©code\n",
    "            classifier (nn.Module): Le classifieur\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, source_input_ids, target_input_ids):\n",
    "        encoder_embeddings, encoder_outputs, hidden = self.encoder(source_input_ids)    # encodage => in, out, hidden_state\n",
    "        _, output, hidden = self.decoder(target_input_ids, hidden, encoder_embeddings=encoder_embeddings, encoder_outputs=encoder_outputs)\n",
    "                                                                                        # decodage => out, hidden\n",
    "        return self.classifier(output), hidden, encoder_embeddings, encoder_outputs     # etat cach√©, embedding, sortie de l'encodeur\n",
    "\n",
    "    def decoder_step(self, inputs, hidden, encoder_embeddings, encoder_outputs):\n",
    "        _, output, hidden = self.decoder(inputs, hidden, encoder_outputs=encoder_outputs, encoder_embeddings=encoder_embeddings)\n",
    "        return self.classifier(output), hidden\n",
    "\n",
    "def generate(tokenizer, model: Seq2Seq, document: Union[str, List[str]], maxlength=50):\n",
    "    \"\"\"G√©n√®re une suite de tokens en utilisant la distribution de probabilit√© du mod√®le\"\"\"\n",
    "\n",
    "    if isinstance(document, str):\n",
    "        document = [document]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        toks = tokenizer(document, return_tensors=\"pt\", return_length=True, padding=True)\n",
    "        \n",
    "        x = toks[\"input_ids\"].T.contiguous().to(device)\n",
    "\n",
    "        # S√©equences g√©n√©r√©es\n",
    "        generated = [[] for _ in range(len(document))]\n",
    "        lengths = [maxlength for _ in range(len(document))]\n",
    "\n",
    "        bos = torch.LongTensor([[tokenizer.bos_token_id]]).tile(1, len(document)).to(device)\n",
    "        y_t, s_t, encoder_embeddings, encoder_outputs = model(x, bos) # application du mod√®le\n",
    "\n",
    "        for length in range(maxlength):\n",
    "            w_t = torch.distributions.categorical.Categorical(logits=y_t[-1]).sample()\n",
    "\n",
    "            w_t_cpu = w_t.cpu().numpy()\n",
    "            for ix, (g, w) in enumerate(zip(generated, w_t_cpu)):\n",
    "                g.append(int(w))\n",
    "                if w == tokenizer.eos_token_id:\n",
    "                    lengths[ix] = min(lengths[ix], length)\n",
    "\n",
    "\n",
    "            y_t, s_t = model.decoder_step(w_t.unsqueeze(0), s_t, encoder_embeddings, encoder_outputs)\n",
    "\n",
    "        return [tokenizer.decode(s[:lengths[ix]]) for ix, s in enumerate(generated)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCHSIZE = 128\n",
    "TEST_BATCHSIZE = 128\n",
    "\n",
    "def computeloss(batch, model, loss):\n",
    "    \"\"\"Calcule le co√ªt du mod√®le sur un batch, ainsi que des m√©triques\"\"\"\n",
    "    source_input_ids = getdata(batch, \"document\", device)\n",
    "    target_input_ids = getdata(batch, \"summary\", device)\n",
    "    yhat, *args = model(source_input_ids, target_input_ids[:-1])\n",
    "    predicted, reference = yhat.view(-1, yhat.shape[2]), target_input_ids[1:].view(-1)\n",
    "    return loss(predicted, reference)\n",
    "\n",
    "def train_seq2seq(model: Seq2Seq, epochs: int, datasets, *, val_steps=1):\n",
    "    \"\"\"Entra√Ænement des mod√®les\n",
    "    \n",
    "    Args:\n",
    "        model (Seq2Seq): le mod√®le √† entra√Æner\n",
    "        epochs (int): le nombre d'√©poques d'entra√Ænement\n",
    "        val_steps (int, optional): le nombre d'√©poques entre chaque calcul de performance sur le jeu de validation\n",
    "    \"\"\"\n",
    "    print(f\"Training {model.name}\")\n",
    "    \n",
    "    # On nettoie le rep. de log\n",
    "    tbpath = f\"{TB_PATH}/{model.name}\"\n",
    "    shutil.rmtree(tbpath, ignore_errors = True)\n",
    "    writer = SummaryWriter(tbpath)\n",
    "    \n",
    "    optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_loader = DataLoader(datasets[\"train\"], TRAIN_BATCHSIZE, shuffle=True)\n",
    "    test_loader = DataLoader(datasets[\"test\"], TEST_BATCHSIZE, shuffle=False)\n",
    "    loss = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_type_id)\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        cumloss, count =  0, 0\n",
    "        model.train()\n",
    "        for ix, batch in enumerate(train_loader):\n",
    "            optim.zero_grad()\n",
    "            l = computeloss(batch, model, loss)\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            batchlen = len(batch[\"document\"])\n",
    "            cumloss += l.item() * batchlen\n",
    "            count += batchlen\n",
    "\n",
    "        writer.add_scalar('loss/train', cumloss/count, epoch)\n",
    "\n",
    "        if epoch % val_steps == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                cumloss, count = 0, 0\n",
    "                for batch in test_loader:\n",
    "                    l = computeloss(batch, model, loss)\n",
    "                    batchlen = len(batch[\"document\"])\n",
    "                    \n",
    "                    # Compute metrics\n",
    "                    predictions = generate(tokenizer, model, batch[\"document\"])\n",
    "                    rouge.add_batch(predictions=predictions, references=batch[\"summary\"])\n",
    "\n",
    "                    cumloss += l * batchlen\n",
    "                    count += batchlen\n",
    "    \n",
    "                for key, value in rouge.compute().items():\n",
    "                    writer.add_scalar(f\"{key}/test\", value.mid.fmeasure, epoch)\n",
    "                writer.add_scalar(f'loss/test', cumloss/count, epoch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant reprendre le code du LSTM vu en 4.1 et l'adapter pour la t√¢che en respectant le prototype \n",
    "donn√© par `RNNBase` - pour l'instant, ignorez `encoder_outputs` et `encoder_embeddings`, ils seront utiles dans\n",
    "la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprendre le code du RNN et l'adapter\n",
    "\n",
    "class LSTM(RNNBase):\n",
    "#  TODO \n",
    "\n",
    "#  Maintenant, cr√©ez le model Seq2Seq en utilisant deux RNNs (enc+dec)\n",
    "#  TODO \n",
    "\n",
    "# On regarde la g√©n√©ration (cela doit √™tre totalement al√©atoire pour l'instant...)\n",
    "print(raw_datasets[\"test\"][5][\"document\"][:400])\n",
    "print(\"--->\")\n",
    "print(generate(tokenizer, model, raw_datasets[\"test\"][5][\"document\"]))\n",
    "\n",
    "# Maintenant, on peut entra√Æner notre mod√®le\n",
    "# (model est un Seq2Seq)\n",
    "train_seq2seq(model, 50, raw_datasets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant voir les s√©quences g√©n√©r√©es en utilisant la m√©thode `generate` adapt√©e aux nouvelles sorties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_datasets[\"test\"][5][\"document\"][:400])\n",
    "print(\"--->\")\n",
    "print(generate(tokenizer, model, raw_datasets[\"test\"][5][\"document\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"background: green; padding: 3px; color: white\">Exercice 2 : Ajouter de l'attention</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant faire un pas de plus vers les transformers... en utilisant un m√©canisme d'attention.\n",
    "\n",
    "Pour faire cela, nous allons tout d'abord calculer une attention sur les sorties de l'encodeur $o_{1\\ldots N}$ (tenseur temps x batch x dim. espace latent), et utiliser une combinaison des embeddings des entr√©es $x_{1\\ldots M}$ (tenseur temps x batch x dim. embeddings). \n",
    "\n",
    "√âtant donn√© les sorties du d√©codeur, $z_{1\\ldots M}$, l'attention est calcul√©e de la mani√®re suivante :\n",
    "\n",
    "1. On calcule les \"clefs\" $k_{1\\ldots N}$ en utilisant une transformation lin√©aire des sorties de l'encodeur (dimension $d$ arbitraire)\n",
    "1. On calcule les \"questions\" $q_{1\\ldots M}$ en utilisant une transformation lin√©aire des sorties du d√©codeur (m√™me dimension $d$ que les clefs)\n",
    "1. On calcule le produit scalaire de chaque clef $k_{i,j}$ (vecteur de dimension $d$) avec chaque question $q_{k, j}$ (pour un √©chantillon $j$) puis normalisons avec `softmax` pour obtenir une distribution de probabilit√© conditionnelle que le token $k$ du d√©codeur utilise le token $i$ de l'encodeur $p_j(i|k)$ : \n",
    "   $$ p_j(k|i) \\propto \\exp\\left( k_{i,j} \\cdot q_{k, j} \\right)$$\n",
    "1. On modifie la sortie du d√©codeur en ajoutant une combinaison convexe des embeddings de l'encodeur (cela permet d'utiliser des mots du vocabulaire utilis√©e dans le texte source plus facilement) :\n",
    "   $$ z^{\\prime}_{i, j} = z_{i, j} + \\sum_{k=1}^{N} p_j(k|i) v(x_k) $$ \n",
    "   o√π $v$ est une fonction de transformation (vous pouvez utiliser l'identit√© si la dimension des sorties du RNN est la m√™me que celle des embeddings)\n",
    "\n",
    "Cr√©ez une classe sp√©cifique pour le d√©codeur et entra√Ænez votre nouveau mod√®le, puis visualisez les r√©sultats - vous devriez obtenir une diminution du co√ªt en entra√Ænement (et en validation), ainsi qu'une qualit√© un peu meilleure des sorties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un nouveau d√©codeur qui utilise de l'attention sur l'encodeur\n",
    "\n",
    "# correction pr√©c√©dente\n",
    "class LSTM(RNNBase):\n",
    "    def __init__(self, vocab_size, embeddings_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.embeddings = nn.Embedding(vocab_size, embeddings_dim)\n",
    "        self.rnn = nn.LSTM(embeddings_dim, hidden_dim)        \n",
    "    \n",
    "    def forward(self, x, h_0=None, *, encoder_outputs=None, encoder_embeddings=None):\n",
    "        x = self.embeddings(x)\n",
    "        output, hidden = self.rnn(x, h_0)\n",
    "        return x, output, hidden\n",
    "\n",
    "\n",
    "class LSTMWithAttention(nn.Module):\n",
    "#  TODO \n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "\n",
    "embeddings = nn.Embedding(vocab_size, 100)\n",
    "encoder = LSTM(embeddings, 100)\n",
    "decoder = LSTMWithAttention(embeddings, 100)\n",
    "classifier = nn.Linear(100, vocab_size)\n",
    "\n",
    "model_att = Seq2Seq(\"lstm-att\", encoder, decoder, classifier)\n",
    "train_seq2seq(model_att, 50, raw_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du mod√®le sur un exemple\n",
    "\n",
    "d = raw_datasets[\"test\"][5][\"document\"]\n",
    "print(d)\n",
    "print(\"--->\")\n",
    "print(generate(tokenizer, model_att, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  TODO )\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "902a52bcf4503a473db011f1937bdfe17613b08622219712e0110e48c4958c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
