{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G√©n√©ration condtionn√©e (Seq2Seq) avec des RNNs et de l'attention\n",
    "\n",
    "Dans le TP pr√©c√©dent, nous avons utilis√© des RNNs pour g√©n√©rer du texte \"libre\" - ou bien conditionn√© par\n",
    "le d√©but de la s√©quence. Pour certaines t√¢ches, comme par exemple la traduction ou la cr√©ation\n",
    "de l√©gendes pour les images, il peut √™tre int√©ressant de traiter de mani√®re\n",
    "diff√©rente la repr√©sentation des donn√©es en entr√©es et en sortie.\n",
    "\n",
    "De plus, afin d'am√©liorer les performance des mod√®les, les RNNs peuvent utiliser une \"m√©moire\" - dans\n",
    "notre cas, il s'agit du texte en entr√©e. Cette id√©e est reprise dans les transformers que nous \n",
    "verrons dans le module suivant.\n",
    "\n",
    "Dans cette partie, nous allons introduire deux nouveaut√©s par rapport aux RNNs du TP pr√©c√©dent :\n",
    "\n",
    "1. Nous allons utiliser un encodeur et un d√©codeur (seq2seq) avec des param√®tres distincts\n",
    "1. Nous allons utiliser un m√©canisme d'attention\n",
    "\n",
    "Les prochaines cellules permettent de charger et pr√©parer les donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir /Users/vguigue/Documents/Cours/Agro-IODAA/deep/notebooks/xp/seq2seq/logs\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "from typing import Tuple, Any, List, Union\n",
    "import shutil\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from tqdm.autonotebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "cachepath = os.path.expanduser('~/.local/data')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BASEPATH = Path(\"xp/seq2seq\")\n",
    "TB_PATH =  BASEPATH / \"logs\"\n",
    "TB_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"tensorboard --logdir {Path(TB_PATH).absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons utiliser le m√™me jeu de donn√©es que dans le carnet pr√©c√©dent, mais en utilisant cette fois-ci les deux textes (document et r√©sum√©)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xsum (/Users/vguigue/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913e17240ab5498da2e0e43c405197cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_h/xz4nr0h53dj3x3tygxjnzl540000gn/T/ipykernel_30891/3347493791.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "# On prend juste 10% de la validation pour aller plus vite\n",
    "raw_datasets = load_dataset(\"xsum\", split={\"train\": \"train[:10%]\", \"validation\": \"validation[:5%]\", \"test\": \"validation[5%:]\"})\n",
    "\n",
    "# Dans le cadre du r√©sum√©, nous allons utiliser la m√©trique \"rouge\"\n",
    "rouge = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculates average rouge scores for a list of hypotheses and references\n",
      "Args:\n",
      "    predictions: list of predictions to score. Each prediction\n",
      "        should be a string with tokens separated by spaces.\n",
      "    references: list of reference for each prediction. Each\n",
      "        reference should be a string with tokens separated by spaces.\n",
      "    rouge_types: A list of rouge types to calculate.\n",
      "        Valid names:\n",
      "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
      "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
      "        `\"rougeLSum\"`: rougeLsum splits text using `\"\n",
      "\"`.\n",
      "        See details in https://github.com/huggingface/datasets/issues/617\n",
      "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
      "    use_aggregator: Return aggregates if this is set to True\n",
      "Returns:\n",
      "    rouge1: rouge_1 (precision, recall, f1),\n",
      "    rouge2: rouge_2 (precision, recall, f1),\n",
      "    rougeL: rouge_l (precision, recall, f1),\n",
      "    rougeLsum: rouge_lsum (precision, recall, f1)\n",
      "Examples:\n",
      "\n",
      "    >>> rouge = datasets.load_metric('rouge')\n",
      "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
      "    >>> references = [\"hello there\", \"general kenobi\"]\n",
      "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
      "    >>> print(list(results.keys()))\n",
      "    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
      "    >>> print(results[\"rouge1\"])\n",
      "    AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))\n",
      "    >>> print(results[\"rouge1\"].mid.fmeasure)\n",
      "    1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rouge.inputs_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plut√¥t que d'utiliser un vocabulaire entra√Æn√© sur les textes en apprentissage, nous allons utiliser ici un vocabulaire plus\n",
    "large qui a √©t√© utilis√© pour BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b076d0992af4e77a99df3af48cd1e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91bc402df15a4f609b86a944ff990af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd48f2f80d944958226e71d91024dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7daa9a106ba8491985bbbb861ecb34b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', bos_token=\"<bos>\", eos_token=\"<eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[30522,  2023,  2003,  1996,  2034,  6254, 30523,     0],\n",
      "        [30522,  2628,  2011,  1996,  2279,  2028, 30523,     0],\n",
      "        [30522,  1998,  1996,  2345,  3793,  2003,  2182, 30523]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<bos> this is the first document <eos> [PAD]',\n",
       " '<bos> followed by the next one <eos> [PAD]',\n",
       " '<bos> and the final text is here <eos>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [\"<bos> This is the first document <eos>\", \"<bos> followed by the next one <eos>\", \"<bos> and the final text is here <eos>\"]\n",
    "r = tokenizer(batch,  truncation=True, add_special_tokens=False, return_token_type_ids=False, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "print(r)\n",
    "\n",
    "[\" \".join(tokenizer.convert_ids_to_tokens(row)) for row in r[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[30522, 30522],\n",
       "         [ 4550,  2048],\n",
       "         [ 1011,  7538],\n",
       "         [ 2039,  7793],\n",
       "         [ 3136,  2031],\n",
       "         [ 2024,  2042],\n",
       "         [ 5719,  3908],\n",
       "         [ 2408,  2011],\n",
       "         [ 1996,  2543],\n",
       "         [ 4104,  1999],\n",
       "         [ 6645,  1037],\n",
       "         [ 1998,  6878],\n",
       "         [ 4241, 24912],\n",
       "         [ 2213,  2886],\n",
       "         [19699,  1999],\n",
       "         [ 3111, 10330],\n",
       "         [ 1998,  2103],\n",
       "         [22372,  2803],\n",
       "         [ 2044,  1012],\n",
       "         [ 9451, 30523],\n",
       "         [ 3303,     0],\n",
       "         [ 2011,     0],\n",
       "         [ 4040,     0],\n",
       "         [ 3581,     0],\n",
       "         [ 1012,     0],\n",
       "         [30523,     0]]),\n",
       " torch.Size([26, 2]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getdata(batch, what: str, device):\n",
    "    \"\"\"Fonction utilitaire pour r√©duire la taille des donn√©es en fonction du batch\"\"\"\n",
    "\n",
    "    r = tokenizer([f\"<bos> {t} <eos>\" for t in batch[what]],  truncation=True, add_special_tokens=False, return_token_type_ids=False, padding=True, return_tensors=\"pt\", max_length=512)\n",
    "    # Renvoie dans le format RNN (temps en premier)\n",
    "    return (r[\"input_ids\"].T).to(device).contiguous()\n",
    "\n",
    "# Exemple\n",
    "loader = DataLoader(raw_datasets[\"train\"], batch_size=2)\n",
    "input_ids = getdata(next(iter(loader)), \"summary\", device)\n",
    "input_ids, input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"background: green; padding: 3px; color: white\">Exercice 1 : impl√©menter un seq2seq</span>\n",
    "\n",
    "La cellule suivante permet de d√©finir:\n",
    "\n",
    "- `RNNBase` qui est le prototype qui sera utilis√© par tous vos RNNs (encodeurs et d√©codeurs)\n",
    "- `Seq2Seq` qui est un mod√®le qui permet de regrouper encodeur, d√©codeur et classifieur (logits de la distribution multinomiale sur les tokens)\n",
    "- `train_seq2seq` qui permet d'apprendre un mod√®le `Seq2Seq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNNBase(nn.Module):\n",
    "    \"\"\"Cette classe sert de base pour tous vos mod√®les r√©currents\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.LongTensor, h_0=None, *, encoder_outputs=None, encoder_embeddings=None) -> Tuple[nn.Module, nn.Module, Any]:\n",
    "        \"\"\"M√©thode principale pour les r√©seaux r√©currents\n",
    "\n",
    "        Les param√®tres `encoder_*` serviront pour l'exercice 2\n",
    "\n",
    "        Args:\n",
    "            x (torch.LongTensor): Un tenseur contenant un batch de s√©quences sous forme d'ID de tokens (temps x batch) \n",
    "            h_0 (Any, optional): √âtat initial √† utiliser.\n",
    "            encoder_outputs (torch.Tensor, optional): Les sorties de l'encodeur\n",
    "            encoder_embeddings (torch.Tensor, optional): Les entr√©es de l'encodeur\n",
    "\n",
    "        Returns:\n",
    "            Tuple[nn.Module, nn.Module, Any]: Renvoie un tuple (embeddings, sorties du RNN, √©tat final)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"Mod√®le Seq2Seq g√©n√©rique\"\"\"\n",
    "\n",
    "    def __init__(self, name: str, encoder: nn.Module, decoder: nn.Module, classifier: nn.Module):\n",
    "        \"\"\"Initialise le mod√®le seq2seq\n",
    "\n",
    "        Args:\n",
    "            name (str): Le nom du mod√®le (pour tensorboard)\n",
    "            encoder (nn.Module): Un RNN qui encode\n",
    "            decoder (nn.Module): Un RNN qui d√©code\n",
    "            classifier (nn.Module): Le classifieur\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, source_input_ids, target_input_ids):\n",
    "        encoder_embeddings, encoder_outputs, hidden = self.encoder(source_input_ids)\n",
    "        _, output, hidden = self.decoder(target_input_ids, hidden, encoder_embeddings=encoder_embeddings, encoder_outputs=encoder_outputs)\n",
    "        return self.classifier(output), hidden, encoder_embeddings, encoder_outputs\n",
    "\n",
    "    def decoder_step(self, inputs, hidden, encoder_embeddings, encoder_outputs):\n",
    "        _, output, hidden = self.decoder(inputs, hidden, encoder_outputs=encoder_outputs, encoder_embeddings=encoder_embeddings)\n",
    "        return self.classifier(output), hidden\n",
    "\n",
    "def generate(tokenizer, model: Seq2Seq, document: Union[str, List[str]], maxlength=50):\n",
    "    \"\"\"G√©n√®re une suite de tokens en utilisant la distribution de probabilit√© du mod√®le\"\"\"\n",
    "\n",
    "    if isinstance(document, str):\n",
    "        document = [document]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        toks = tokenizer(document, return_tensors=\"pt\", return_length=True, padding=True)\n",
    "        \n",
    "        x = toks[\"input_ids\"].T.contiguous().to(device)\n",
    "\n",
    "        # S√©equences g√©n√©r√©es\n",
    "        generated = [[] for _ in range(len(document))]\n",
    "        lengths = [maxlength for _ in range(len(document))]\n",
    "\n",
    "        bos = torch.LongTensor([[tokenizer.bos_token_id]]).tile(1, len(document)).to(device)\n",
    "        y_t, s_t, encoder_embeddings, encoder_outputs = model(x, bos)\n",
    "\n",
    "        for length in range(maxlength):\n",
    "            w_t = torch.distributions.categorical.Categorical(logits=y_t[-1]).sample()\n",
    "\n",
    "            w_t_cpu = w_t.cpu().numpy()\n",
    "            for ix, (g, w) in enumerate(zip(generated, w_t_cpu)):\n",
    "                g.append(int(w))\n",
    "                if w == tokenizer.eos_token_id:\n",
    "                    lengths[ix] = min(lengths[ix], length)\n",
    "\n",
    "\n",
    "            y_t, s_t = model.decoder_step(w_t.unsqueeze(0), s_t, encoder_embeddings, encoder_outputs)\n",
    "\n",
    "        return [tokenizer.decode(s[:lengths[ix]]) for ix, s in enumerate(generated)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCHSIZE = 128\n",
    "TEST_BATCHSIZE = 128\n",
    "\n",
    "def computeloss(batch, model, loss):\n",
    "    \"\"\"Calcule le co√ªt du mod√®le sur un batch, ainsi que des m√©triques\"\"\"\n",
    "    source_input_ids = getdata(batch, \"document\", device)\n",
    "    target_input_ids = getdata(batch, \"summary\", device)\n",
    "    yhat, *args = model(source_input_ids, target_input_ids[:-1])\n",
    "    predicted, reference = yhat.view(-1, yhat.shape[2]), target_input_ids[1:].view(-1)\n",
    "    return loss(predicted, reference)\n",
    "\n",
    "def train_seq2seq(model: Seq2Seq, epochs: int, datasets, *, val_steps=1):\n",
    "    \"\"\"Entra√Ænement des mod√®les\n",
    "    \n",
    "    Args:\n",
    "        model (Seq2Seq): le mod√®le √† entra√Æner\n",
    "        epochs (int): le nombre d'√©poques d'entra√Ænement\n",
    "        val_steps (int, optional): le nombre d'√©poques entre chaque calcul de performance sur le jeu de validation\n",
    "    \"\"\"\n",
    "    print(f\"Training {model.name}\")\n",
    "    \n",
    "    # On nettoie le rep. de log\n",
    "    tbpath = f\"{TB_PATH}/{model.name}\"\n",
    "    shutil.rmtree(tbpath, ignore_errors = True)\n",
    "    writer = SummaryWriter(tbpath)\n",
    "    \n",
    "    optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_loader = DataLoader(datasets[\"train\"], TRAIN_BATCHSIZE, shuffle=True)\n",
    "    test_loader = DataLoader(datasets[\"test\"], TEST_BATCHSIZE, shuffle=False)\n",
    "    loss = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_type_id)\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        cumloss, count =  0, 0\n",
    "        model.train()\n",
    "        for ix, batch in enumerate(train_loader):\n",
    "            optim.zero_grad()\n",
    "            l = computeloss(batch, model, loss)\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            batchlen = len(batch[\"document\"])\n",
    "            cumloss += l.item() * batchlen\n",
    "            count += batchlen\n",
    "\n",
    "        writer.add_scalar('loss/train', cumloss/count, epoch)\n",
    "\n",
    "        if epoch % val_steps == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                cumloss, count = 0, 0\n",
    "                for batch in test_loader:\n",
    "                    l = computeloss(batch, model, loss)\n",
    "                    batchlen = len(batch[\"document\"])\n",
    "                    \n",
    "                    # Compute metrics\n",
    "                    predictions = generate(tokenizer, model, batch[\"document\"])\n",
    "                    rouge.add_batch(predictions=predictions, references=batch[\"summary\"])\n",
    "\n",
    "                    cumloss += l * batchlen\n",
    "                    count += batchlen\n",
    "    \n",
    "                for key, value in rouge.compute().items():\n",
    "                    writer.add_scalar(f\"{key}/test\", value.mid.fmeasure, epoch)\n",
    "                writer.add_scalar(f'loss/test', cumloss/count, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant reprendre le code du LSTM vu en 4.1 et l'adapter pour la t√¢che en respectant le prototype \n",
    "donn√© par `RNNBase` - pour l'instant, ignorez `encoder_outputs` et `encoder_embeddings`, ils seront utiles dans\n",
    "la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (870 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addressing the party's UK conference, Ms Dugdale restated her support for the tax policy her party ran on in May.\n",
      "She wants a 50p tax on those earning over ¬£150,000, and a penny increase in income tax to pay for public services.\n",
      "The conference will vote on Tuesday on plans to give the Scottish and Welsh leaders more power over their parties.\n",
      "The proposals would allow Kezia Dugdale and Welsh First \n",
      "--->\n",
      "['supportedax participating at benedict [unused197] demographics clockwise threaded simulcast scoutsrianzog 71neil chavez dockyard cargo refuse federer staring diplomaticplane charts namibia didnli palestinians mechanical saltaton mating seen monitor cheekssch bought interfering molly 1718 viktor Êµ∑rriganurgent whipping passes membranes abandon pratt forty']\n",
      "Training lstm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df332bc1f2c4b2f9683b97649825c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# [[student]] Reprendre le code du RNN et l'adapter\n",
    "class LSTM(RNNBase):\n",
    "    def __init__(self, vocab_size, embeddings_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.embeddings = nn.Embedding(vocab_size, embeddings_dim)\n",
    "        self.rnn = nn.LSTM(embeddings_dim, hidden_dim)        \n",
    "    \n",
    "    def forward(self, x, h_0=None, *, encoder_outputs=None, encoder_embeddings=None):\n",
    "        x = self.embeddings(x)\n",
    "        output, hidden = self.rnn(x, h_0)\n",
    "        return x, output, hidden \n",
    "# [[/student]]\n",
    "\n",
    "\n",
    "# [[student]] Maintenant, cr√©ez le model Seq2Seq en utilisant deux RNNs\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "encoder = LSTM(vocab_size, 100, 100)\n",
    "decoder = LSTM(vocab_size, 100, 100)\n",
    "classifier = nn.Linear(100, vocab_size)\n",
    "model = Seq2Seq(\"lstm\", encoder, decoder, classifier).to(device)\n",
    "# [[/student]]\n",
    "\n",
    "# On regarde la g√©n√©ration (cela doit √™tre totalement al√©atoire pour l'instant...)\n",
    "print(raw_datasets[\"test\"][5][\"document\"][:400])\n",
    "print(\"--->\")\n",
    "print(generate(tokenizer, model, raw_datasets[\"test\"][5][\"document\"]))\n",
    "\n",
    "# Maintenant, on peut entra√Æner notre mod√®le\n",
    "# (model est un Seq2Seq)\n",
    "train_seq2seq(model, 50, raw_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant voir les s√©quences g√©n√©r√©es en utilisant la m√©thode `generate` adapt√©e aux nouvelles sorties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addressing the party's UK conference, Ms Dugdale restated her support for the tax policy her party ran on in May.\n",
      "She wants a 50p tax on those earning over ¬£150,000, and a penny increase in income tax to pay for public services.\n",
      "The conference will vote on Tuesday on plans to give the Scottish and Welsh leaders more power over their parties.\n",
      "The proposals would allow Kezia Dugdale and Welsh First \n",
      "--->\n",
      "['scottishjet arsenalsneys ) can help those raped a rise of revenge and boeing from other africa leaders until beating people under \" \" to the bbc councils in the ex election.']\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"test\"][5][\"document\"][:400])\n",
    "print(\"--->\")\n",
    "print(generate(tokenizer, model, raw_datasets[\"test\"][5][\"document\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"background: green; padding: 3px; color: white\">Exercice 2 : Ajouter de l'attention</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant faire un pas de plus vers les transformers... en utilisant un m√©canisme d'attention.\n",
    "\n",
    "Pour faire cela, nous allons tout d'abord calculer une attention sur les sorties de l'encodeur $o_{1\\ldots N}$ (tenseur temps x batch x dim. espace latent), et utiliser une combinaison des embeddings des entr√©es $x_{1\\ldots M}$ (tenseur temps x batch x dim. embeddings). \n",
    "\n",
    "√âtant donn√© les sorties du d√©codeur, $z_{1\\ldots M}$, l'attention est calcul√©e de la mani√®re suivante :\n",
    "\n",
    "1. On calcule les \"clefs\" $k_{1\\ldots N}$ en utilisant une transformation lin√©aire des sorties de l'encodeur (dimension $d$ arbitraire)\n",
    "1. On calcule les \"questions\" $q_{1\\ldots M}$ en utilisant une transformation lin√©aire des sorties du d√©codeur (m√™me dimension $d$ que les clefs)\n",
    "1. On calcule le produit scalaire de chaque clef $k_{i,j}$ (vecteur de dimension $d$) avec chaque question $q_{k, j}$ (pour un √©chantillon $j$) puis normalisons avec `softmax` pour obtenir une distribution de probabilit√© conditionnelle que le token $k$ du d√©codeur utilise le token $i$ de l'encodeur $p_j(i|k)$ : \n",
    "   $$ p_j(k|i) \\propto \\exp\\left( k_{i,j} \\cdot q_{k, j} \\right)$$\n",
    "1. On modifie la sortie du d√©codeur en ajoutant une combinaison convexe des embeddings de l'encodeur (cela permet d'utiliser des mots du vocabulaire utilis√©e dans le texte source plus facilement) :\n",
    "   $$ z^{\\prime}_{i, j} = z_{i, j} + \\sum_{k=1}^{N} p_j(k|i) v(x_k) $$ \n",
    "   o√π $v$ est une fonction de transformation (vous pouvez utiliser l'identit√© si la dimension des sorties du RNN est la m√™me que celle des embeddings)\n",
    "\n",
    "Cr√©ez une classe sp√©cifique pour le d√©codeur et entra√Ænez votre nouveau mod√®le, puis visualisez les r√©sultats - vous devriez obtenir une diminution du co√ªt en entra√Ænement (et en validation), ainsi qu'une qualit√© un peu meilleure des sorties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lstm-att\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f8fb5324b141c8bb67e2bc2c412e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4123 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addressing the party's UK conference, Ms Dugdale restated her support for the tax policy her party ran on in May.\n",
      "She wants a 50p tax on those earning over ¬£150,000, and a penny increase in income tax to pay for public services.\n",
      "The conference will vote on Tuesday on plans to give the Scottish and Welsh leaders more power over their parties.\n",
      "The proposals would allow Kezia Dugdale and Welsh First Minister Carwyn Jones the power to appoint a representative to the party's UK national executive committee (NEC).\n",
      "The plans would also give the Scottish and Welsh parties more autonomy.\n",
      "Sources have said there was an attempt to \"unpick\" the plans by removing the extra seats from the package. This was defeated, but it is possible there could be a second attempt prior to Tuesday's vote.\n",
      "Speaking at the weekend, Len McCluskey, head of the UK's biggest trade union Unite, said Ms Dugdale should not have the power to appoint Scotland's representative on the NEC.\n",
      "He said that while there was wide support for the autonomy proposals, coupling them with the NEC seat plans had created a \"difficult position\".\n",
      "Ms Dugdale has said she is confident the full package will be passed.\n",
      "It appears Kezia Dugdale and Jeremy Corbyn are involved in their first dispute since his re-election.\n",
      "A series of reforms to the way the Labour Party works are due to be passed on Tuesday.\n",
      "The package includes autonomy plans praised by Ms Dugdale last week. This part is uncontroversial and supported by everyone I've spoken to.\n",
      "But there is a row over another part. The reforms would also allow Ms Dugdale - and her Welsh counterpart Carwyn Jones - to appoint someone to represent them on the party's UK executive.\n",
      "Mr Corbyn's supporters think the extra NEC seats could be used to try and undermine him as the battle for the party's future plays out. They want to postpone the plan.\n",
      "For now, it appears Ms Dugdale will get her way. But it's possible there will be another attempt to scupper the NEC plan tomorrow.\n",
      "In her address to the conference in Liverpool on Monday afternoon, the Scottish Labour leadersaid her party will not support \"another austerity budget\" in Scotland.\n",
      "The speech is Ms Dugdale's first to conference since the Holyrood elections earlier this year, when Labour fell to third place in terms of the seats it holds in the Scottish parliament, behind the Conservatives.\n",
      "MSPs at Holyrood will be responsible for setting income tax in Scotland from April 2017, but First Minister Nicola Sturgeon has already rejected increasing the basic rate for the five year lifetime of the parliament, as well as ruling out upping the top rate in the first year.\n",
      "Ms Dugdale said her party would \"place amendments to introduce a 50p tax on those earning over ¬£150,000 and to add a penny to income tax to pay for public services\" when the Scottish government brings forward its budget for 2017-18.\n",
      "She said: \"With the full range of powers the Scottish Parliament now has, the SNP government faces a clear choice.\n",
      "\"Accept a Tory budget from Westminster, or go our own way with proposals to grow the Scottish economy and protect our schools and hospitals.\n",
      "\"If the SNP minority government do not accept these proposals, and try to force another austerity budget through Holyrood, we will vote against it.\n",
      "\"If they want support, they'll need to look to the Tories for that. Labour will not help the SNP pass an austerity budget.\"\n",
      "Ms Dugdale also attacked Ms Sturgeon over her stance on independence, saying Scottish Labour would not support a second referendum during the current Holyrood term.\n",
      "She said: \"We do not need the risk and uncertainty of another independence referendum.\n",
      "\"As we face negotiations on our membership of the EU and real threats to the future of our public services, we cannot afford our government to take their eye off the ball.\n",
      "\"With so many challenges facing Scotland in the future, we should not return to the divisions of the past.\n",
      "\"My message to Nicola Sturgeon is this: first minister, our country is already divided enough. Do not divide us again.\"\n",
      "--->\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/bpiwowar/teaching/rital/2022/cdiscount/tp-bp/module-4/4.2-seq2seq-attention.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bpiwowar/teaching/rital/2022/cdiscount/tp-bp/module-4/4.2-seq2seq-attention.ipynb#ch0000016?line=50'>51</a>\u001b[0m \u001b[39mprint\u001b[39m(raw_datasets[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m5\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bpiwowar/teaching/rital/2022/cdiscount/tp-bp/module-4/4.2-seq2seq-attention.ipynb#ch0000016?line=51'>52</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m--->\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bpiwowar/teaching/rital/2022/cdiscount/tp-bp/module-4/4.2-seq2seq-attention.ipynb#ch0000016?line=52'>53</a>\u001b[0m \u001b[39mprint\u001b[39m(generate(tokenizer, model, raw_datasets[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m5\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# [[student]] Cr√©er un nouveau d√©codeur qui utilise de l'attention sur l'encodeur\n",
    "class LSTM(RNNBase):\n",
    "    def __init__(self, vocab_size, embeddings_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.embeddings = nn.Embedding(vocab_size, embeddings_dim)\n",
    "        self.rnn = nn.LSTM(embeddings_dim, hidden_dim)        \n",
    "    \n",
    "    def forward(self, x, h_0=None, *, encoder_outputs=None, encoder_embeddings=None):\n",
    "        x = self.embeddings(x)\n",
    "        output, hidden = self.rnn(x, h_0)\n",
    "        return x, output, hidden\n",
    "\n",
    "\n",
    "class LSTMWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embeddings_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.embeddings = nn.Embedding(vocab_size, embeddings_dim)\n",
    "        self.rnn = nn.LSTM(embeddings_dim, hidden_dim)        \n",
    "\n",
    "        self.keys = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.queries = nn.Linear(hidden_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x, h_0=None, *, encoder_outputs=None, encoder_embeddings=None):\n",
    "        x = self.embeddings(x)\n",
    "        output, hidden = self.rnn(x, h_0)\n",
    "\n",
    "        # We use the output to guide\n",
    "        queries = self.queries(output) # Shape time_1 x batch x hidden_dim\n",
    "        keys = self.keys(encoder_outputs) # Shape Shape time_2 x batch x hidden_dim\n",
    "\n",
    "        inners = (queries.permute(1,0,2) @ keys.permute(1, 2, 0))\n",
    "        probs = inners.permute(2, 0, 1).softmax(dim=0)\n",
    "\n",
    "        time_1 = len(queries)\n",
    "        hdim = encoder_embeddings.shape[-1]\n",
    "        p_values = encoder_embeddings.unsqueeze(-1).expand((-1, -1, -1, time_1)) * probs.unsqueeze(-2).expand((-1,-1, hdim, -1))\n",
    "        output = p_values.permute(3, 1, 2, 0).sum(-1) + output\n",
    "\n",
    "        return x, output, hidden\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "\n",
    "embeddings = nn.Embedding(vocab_size, 100)\n",
    "encoder = LSTM(embeddings, 100)\n",
    "decoder = LSTMWithAttention(embeddings, 100)\n",
    "classifier = nn.Linear(100, vocab_size)\n",
    "\n",
    "model_att = Seq2Seq(\"lstm-att\", encoder, decoder, classifier)\n",
    "train_seq2seq(model_att, 50, raw_datasets)\n",
    "# [[/student]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addressing the party's UK conference, Ms Dugdale restated her support for the tax policy her party ran on in May.\n",
      "She wants a 50p tax on those earning over ¬£150,000, and a penny increase in income tax to pay for public services.\n",
      "The conference will vote on Tuesday on plans to give the Scottish and Welsh leaders more power over their parties.\n",
      "The proposals would allow Kezia Dugdale and Welsh First Minister Carwyn Jones the power to appoint a representative to the party's UK national executive committee (NEC).\n",
      "The plans would also give the Scottish and Welsh parties more autonomy.\n",
      "Sources have said there was an attempt to \"unpick\" the plans by removing the extra seats from the package. This was defeated, but it is possible there could be a second attempt prior to Tuesday's vote.\n",
      "Speaking at the weekend, Len McCluskey, head of the UK's biggest trade union Unite, said Ms Dugdale should not have the power to appoint Scotland's representative on the NEC.\n",
      "He said that while there was wide support for the autonomy proposals, coupling them with the NEC seat plans had created a \"difficult position\".\n",
      "Ms Dugdale has said she is confident the full package will be passed.\n",
      "It appears Kezia Dugdale and Jeremy Corbyn are involved in their first dispute since his re-election.\n",
      "A series of reforms to the way the Labour Party works are due to be passed on Tuesday.\n",
      "The package includes autonomy plans praised by Ms Dugdale last week. This part is uncontroversial and supported by everyone I've spoken to.\n",
      "But there is a row over another part. The reforms would also allow Ms Dugdale - and her Welsh counterpart Carwyn Jones - to appoint someone to represent them on the party's UK executive.\n",
      "Mr Corbyn's supporters think the extra NEC seats could be used to try and undermine him as the battle for the party's future plays out. They want to postpone the plan.\n",
      "For now, it appears Ms Dugdale will get her way. But it's possible there will be another attempt to scupper the NEC plan tomorrow.\n",
      "In her address to the conference in Liverpool on Monday afternoon, the Scottish Labour leadersaid her party will not support \"another austerity budget\" in Scotland.\n",
      "The speech is Ms Dugdale's first to conference since the Holyrood elections earlier this year, when Labour fell to third place in terms of the seats it holds in the Scottish parliament, behind the Conservatives.\n",
      "MSPs at Holyrood will be responsible for setting income tax in Scotland from April 2017, but First Minister Nicola Sturgeon has already rejected increasing the basic rate for the five year lifetime of the parliament, as well as ruling out upping the top rate in the first year.\n",
      "Ms Dugdale said her party would \"place amendments to introduce a 50p tax on those earning over ¬£150,000 and to add a penny to income tax to pay for public services\" when the Scottish government brings forward its budget for 2017-18.\n",
      "She said: \"With the full range of powers the Scottish Parliament now has, the SNP government faces a clear choice.\n",
      "\"Accept a Tory budget from Westminster, or go our own way with proposals to grow the Scottish economy and protect our schools and hospitals.\n",
      "\"If the SNP minority government do not accept these proposals, and try to force another austerity budget through Holyrood, we will vote against it.\n",
      "\"If they want support, they'll need to look to the Tories for that. Labour will not help the SNP pass an austerity budget.\"\n",
      "Ms Dugdale also attacked Ms Sturgeon over her stance on independence, saying Scottish Labour would not support a second referendum during the current Holyrood term.\n",
      "She said: \"We do not need the risk and uncertainty of another independence referendum.\n",
      "\"As we face negotiations on our membership of the EU and real threats to the future of our public services, we cannot afford our government to take their eye off the ball.\n",
      "\"With so many challenges facing Scotland in the future, we should not return to the divisions of the past.\n",
      "\"My message to Nicola Sturgeon is this: first minister, our country is already divided enough. Do not divide us again.\"\n",
      "--->\n",
      "['presidentgo mickey should bea winds were claimed as he was refusedack on swansea as an closest intervention out orders $ three year.']\n"
     ]
    }
   ],
   "source": [
    "# Test du mod√®le sur un exemple\n",
    "\n",
    "d = raw_datasets[\"test\"][5][\"document\"]\n",
    "print(d)\n",
    "print(\"--->\")\n",
    "print(generate(tokenizer, model_att, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('torch-nightly')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0539fa28f95d3c31ab10ef7831d62bf0e47751f442c28c9f64a267247464e7da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
