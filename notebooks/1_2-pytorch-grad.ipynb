{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TE2ItlsI956"
   },
   "source": [
    "# Séance 1 :  Deep Learning - Introduction à Pytorch \n",
    "\n",
    "Les notebooks sont très largement inspirés des cours de **N. Baskiotis et B. Piwowarski**. Ils peuvent être complétés efficacement par les tutoriels *officiels* présents sur le site de pytorch:\n",
    "https://pytorch.org/tutorials/\n",
    "\n",
    "Au niveau de la configuration, toutes les installations doivent fonctionner sur Linux et Mac. Pour windows, ça peut marcher avec Anaconda à jour... Mais il est difficile de récupérer les problèmes.\n",
    "\n",
    "* Aide à la configuration des machines: [lien](https://dac.lip6.fr/master/environnement-deep/)\n",
    "* Alternative 1 à Windows: installer Ubuntu sous Windows:  [Ubuntu WSL](https://ubuntu.com/wsl)\n",
    "* Alternative 2: travailler sur Google Colab (il faut un compte gmail + prendre le temps de comprendre comment accéder à des fichers) [Colab](https://colab.research.google.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Y9YOOHHhJKY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"La version de torch est : \",torch.__version__)\n",
    "print(\"Le calcul GPU est disponible ? \", torch.cuda.is_available())\n",
    "\n",
    "# pour les possesseurs de mac M1 avec la dernière version de pytorch:\n",
    "print(\"Le calcul GPU est disponible ? \", torch.backends.mps.is_available())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzhX7D8KKIvt"
   },
   "source": [
    "# B. Autograd et graphe de calcul\n",
    "Un élément central de pytorch est le graphe de calcul : lors du calcul d'une variable, l'ensemble des opérations qui ont servies au calcul sont stockées sous la forme d'un graphe acyclique, dit de *calcul*. Les noeuds internes du graphe représentent les opérations, le noeud terminal le résultat et les racines les variables d'entrées. Ce graphe sert en particulier à calculer les dérivées partielles de la sortie par rapport aux variables d'entrées - en utilisant les règles de dérivations chainées des fonctions composées. \n",
    "Pour cela, toutes les fonctions disponibles dans pytorch comportent un mécanisme, appelé *autograd* (automatique differentiation), qui permet de calculer les dérivées partielles des opérations. \n",
    "\n",
    "## B.1. Différenciation automatique\n",
    "(De manière simplifiée, pour les détails cf [la documentation](https://pytorch.org/docs/stable/notes/extending.html))\n",
    "\n",
    "Toute opération sous pytorch hérite de la classe **Function** et doit définir :\n",
    "* une méthode **forward(\\*args)** : passe avant, calcule le résultat de la fonction appliquée aux arguments \n",
    "* une méthode **backward(\\*args)** : passe arrière, calcule les dérivées partielles par rapport aux entrées. Les arguments de  cette méthode correspondent aux valeurs des dérivées suivantes dans le graphe de calcul. En particulier, il y a autant d'arguments à **backward**  que de sorties pour la méthode **forward** (rétro-propagation : on doit connaître les dérivés qui viennent  en aval du calcul) et autant de sorties que d'arguments dans la méthode **forward** (chaque sortie correspond à  une dérivée partielle par rapport à chaque entrée du module). Le calcul se fait sur les valeurs du dernier appel de **forward**. \n",
    "\n",
    "Par exemple, pour la fonction d'addition  **add(x,y)**, **add.forward(x,y)** renverra **x+y** (l'appel de la fonction est équivalent à l'appel de **forward**) et **add.backward(1)** renverra le couple **(1,1)** (la dérivée par rapport à x, et celle par rapport à y) .\n",
    "\n",
    "En pratique, ce ne sont pas les méthodes de ces fonctions qui sont utilisées, mais des méthodes équivalentes sur les tenseurs. La méthode **backward** d'un tenseur permet de rétro-propager le calcul du gradient sur toutes les variables qui ont servies à son calcul.\n",
    "\n",
    "La valeur du gradient pour chaque dérivée partielle se trouve dans l'attribut **grad** de la variable concernée. \n",
    "\n",
    "Comme c'est un mécanisme lourd, l'autograd n'est pas activé par défaut pour une variable. Afin de l'activer, il faut mettre le flag **requires_grad** de cette variable à vrai. Dès lors, tout calcul qui utilise cette variable sera enregistré dans le graphe de calcul et le gradient sera disponible.\n",
    "\n",
    "\n",
    "Exemple : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_mYVeXMfsTV"
   },
   "outputs": [],
   "source": [
    "a = torch.tensor(1.)\n",
    "# Par défaut, requires_grad est à False\n",
    "print(\"Graphe de calcul ? \",a.requires_grad)\n",
    "# On peut demander à ce que le graphe de calcul soit retenu\n",
    "a.requires_grad = True \n",
    "# Ou lors de la création du tenseur directement\n",
    "b = torch.tensor(2.,requires_grad=True)\n",
    "z = 2*a + b\n",
    "# Calcul des dérivées partielles par rapport à z\n",
    "z.backward()\n",
    "print(\"Dérivée de z/a : \", a.grad.item(),\" z/b :\", b.grad.item())\n",
    "\n",
    "# Si on a oublié de demander le graphe de calcul :\n",
    "a, b = torch.tensor(1.),torch.tensor(2.)\n",
    "z = 2*a+b\n",
    "try: # on sait que ça va provoquer une erreur\n",
    "  z.backward()\n",
    "except Exception as e: # erreur => simple message\n",
    "  print(\"Erreur : \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eApYmHIZa917"
   },
   "source": [
    "## B.2. <span class=\"alert-success\">     Exercice :  Utilisation de backward     </div>\n",
    "* Implémentez (en une ligne) la fonction de coût aux moindres carrés $MSE(\\hat{y},y)=\\frac{1}{2N} \\sum_{i=1}^N\\|\\hat{y_i}-y_i\\|^2$ où $\\hat{y},y$ sont deux matrices de taille $N\\times d$, et $y_i,\\hat{y_i}$ les $i$-èmes vecteurs lignes des matrices.\n",
    "* Engendrez **y,yhat** deux matrices aléatoires de taille $(1,5)$.\n",
    "* Calculez **MSE(y,yhat)**\n",
    "* Calculez à la main le gradient de **MSE** par rapport à **y**, **yhat**\n",
    "* Calculez grâce à pytorch le gradient de **MSE** par rapport à **y** et **yhat** et vérifier le résultat.\n",
    "* Appelez une deuxième fois **MSE** sur les mêmes vecteurs et la méthode **backward**. Qu'observez vous pour le gradient ? Comment l'expliquez vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Rd37M_3gkqu"
   },
   "outputs": [],
   "source": [
    "def MSE(yhat,y):\n",
    "    # Compléter la fonction \n",
    "    ##  TODO \n",
    "    pass\n",
    "\n",
    "y = torch.randn(1,5,requires_grad=True)\n",
    "yhat = torch.randn(1,5,requires_grad=True)\n",
    "mse = MSE(yhat,y)\n",
    "print(\"MSE :\" ,mse)\n",
    "\n",
    "# 1. retro-propager l'erreur\n",
    "# 2. afficher le gradient sur les deux vecteurs et comprendre ce qui se passe\n",
    "# 3. faire une itération supplémentaire et afficher de nouveau\n",
    "\n",
    "##  TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxgCbApIgU5X"
   },
   "source": [
    "\n",
    "## B.3. <span class=\"alert-success\"> Exercice :   Régression linéaire en pytorch </span>\n",
    "\n",
    "* Définissez la fonction **flineaire(x,w,b)** fonction linéaire qui calcule $f(x,w,b)=x.w^t+b$  avec $x\\in \\mathbb{R}^{{n\\times d}},~w\\in\\mathbb{R}^{1,d}, b\\in \\mathbb{R}$\n",
    "* Complétez le code ci-dessous pour réaliser une descente de gradient et apprendre les paramètres optimaux de la regression linéaire : $$w^∗,b^∗=\\text{argmin}_{w,b}\\frac{1}{N} \\sum_{i=1}^N \\|f(x^i,w,b)-y^i\\|^2$$\n",
    "\n",
    "Pour tester votre code, utilisez le jeu de données très classique *housing*, le prix des loyers à housing en fonction de caractéristiques socio-économiques des quartiers. Le code ci-dessous permet de les charger.\n",
    "\n",
    "<span style=\"color:red\"> ATTENTION ! </span>\n",
    "* pour la mise-à-jour des paramètres, <span style=\"color:red\">vous ne pouvez pas faire directement</span> \n",
    "$$w = w-\\epsilon*gradient$$ \n",
    "(pourquoi ?). Vous devez passer par w.data qui permet de ne pas enregistrer les opérations dans le graphe de calcul (ou utiliser la méthode ```.detach()``` d'une variable qui permet de créer une copie détachée du graphe de calcul). \n",
    "* Note: il est aussi possible de faire:\n",
    "    ```\n",
    "    with torch.no_grad():\n",
    "        w -= eps*gradient\n",
    "    ```\n",
    "    * Désactivation temporaire du graph de calcul, on manipule les tensors comme des variables classiques\n",
    "    * ATTENTION à faire des ```-=``` ou ```+=``` => Si vous construisez un nouveau tenseur, il ne se reconnectera pas au graphe de calcul!\n",
    "* l'algorithme doit converger avec la valeur de epsilon fixée; si ce n'est pas le cas, il y a une erreur (la plupart du temps au niveau du calcul du coût).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dArOgSWNTVvb"
   },
   "outputs": [],
   "source": [
    "def flineaire(x,w,b):\n",
    "    ##  TODO \n",
    "    pass\n",
    "\n",
    "## Chargement des données housing (depuis sklearn) et transformation en tensor.\n",
    "# from sklearn.datasets import load_housing # => removed\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing() ## chargement des données\n",
    "housing_x = torch.tensor(housing['data'],dtype=torch.float) # penser à typer les données pour éliminer les incertitudes\n",
    "housing_y = torch.tensor(housing['target'],dtype=torch.float)\n",
    "\n",
    "print(\"Nombre d'exemples : \",housing_x.size(0), \"Dimension : \",housing_x.size(1))\n",
    "print(\"Nom des attributs : \", \", \".join(housing['feature_names']))\n",
    "\n",
    "print(housing_x[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPOCHS = 5000\n",
    "EPS = 1e-7 # que se passe-t-il lorsqu'on joue avec EPS?\n",
    "\n",
    "# initialisation aléatoire de w et b\n",
    "w = torch.randn(1,housing_x.size(1),requires_grad=True)\n",
    "b =  torch.randn(1,1,requires_grad=True)\n",
    "loss_h = [] # sauvegarde des valeurs de loss (pas si trivial!)\n",
    "\n",
    "# boucle de descente de gradient\n",
    "for i in range(EPOCHS):\n",
    "    pass\n",
    "    ## SOLUTION 1: Penser à aller chercher w.data (et sa contrepartie dans le gradient)\n",
    "    # 1. Construire la loss (+stocker la valeur dans loss_h)\n",
    "    # 2. Retro-propager\n",
    "    # 3. MAJ des paramètres\n",
    "    # 4. Penser à remettre le gradient à 0 (cf exo précédent)\n",
    "    ##  TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# une seconde version du même code avec l'environnement torch.no_grad()\n",
    "# attention, dans ce cas, le += est obligatoire\n",
    "# code identique (juste changer les 2 lignes de MAJ)\n",
    "\n",
    "EPOCHS = 5000\n",
    "EPS = 1e-7\n",
    "#initialisation aléatoire de w et b\n",
    "w = torch.randn(1,housing_x.size(1),requires_grad=True)\n",
    "b =  torch.randn(1,1,requires_grad=True)\n",
    "loss_h = [] # sauvegarde des valeurs de loss (pas si trivial!)\n",
    "for i in range(EPOCHS):\n",
    "    pass\n",
    "    ## SOLUTION 2: avec torch.no_grad() [toutes les lignes sont identiques, sauf les 2 lignes de MAJ des paramètres]\n",
    "    ##  TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage de l'optimisation\n",
    "plt.figure()\n",
    "plt.plot(loss_h)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"mse loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70il-LA_XGSO"
   },
   "source": [
    "\n",
    "## Optimiseur \n",
    "La descente de gradient représente en fait un code standard puisque les dérivées sont calculées automatiquement et que les variables sont idéntifiées.\n",
    "Pytorch inclut une classe très utile pour la descente de gradient, [torch.optim](https://pytorch.org/docs/stable/optim.html), qui permet :\n",
    "* d'économiser quelques lignes de codes\n",
    "* d'automatiser la mise-à-jour des paramètres \n",
    "* d'abstraire le type de descente de gradient utilisé (sgd,adam, rmsprop, ...)\n",
    "\n",
    "Une liste de paramètres à optimiser est passée à l'optimiseur lors de l'initialisation. La méthode **zero_grad()** permet de remettre le gradient à zéro et la méthode **step()** permet de faire une mise-à-jour des paramètres.\n",
    "\n",
    "Un exemple de code  utilisant l'optimiseur est donné ci-dessous. Testez et comparez les résultats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58sP5ryLeP3Y"
   },
   "outputs": [],
   "source": [
    "Xdim = housing_x.size(1)\n",
    "\n",
    "w = torch.randn(1,Xdim,dtype=torch.float,requires_grad=True)\n",
    "b = torch.randn(1,dtype=torch.float,requires_grad=True)\n",
    "\n",
    "## on optimise selon w et b.  lr est le pas du gradient\n",
    "optim = torch.optim.SGD(params=[w,b],lr=EPS) \n",
    "for i in range(EPOCHS):\n",
    "  loss = MSE(flineaire(housing_x,w,b).view(-1,1),housing_y.view(-1,1))\n",
    "  optim.zero_grad()\n",
    "  loss.backward()\n",
    "  optim.step()  \n",
    "  if i % 100==0:  print(f\"iteration : {i}, loss : {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du sujet à partir de la correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  TODO )\",\" TODO \",\\\n",
    "    txt, flags=re.DOTALL))\n",
    "f2.close()\n",
    "\n",
    "### </CORRECTION> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepLearning fc TP1 2020-2021-correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.15 ('torch-nightly')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "0539fa28f95d3c31ab10ef7831d62bf0e47751f442c28c9f64a267247464e7da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
